{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visoutre/ai-notebooks/blob/main/Stable_Diffusion_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "## Stable Diffusion Batch by [visoutre](https://www.reddit.com/user/visoutre)\n",
        "\n",
        "Location for updates: [Stable Diffusion Batch.ipynb](https://colab.research.google.com/github/visoutre/ai-notebooks/blob/main/Stable_Diffusion_Batch.ipynb)\n",
        "\n",
        "<b> v 2.5.3, last updated 9/13/22 </b>\n",
        "<br>\n",
        "- Expand to view info...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqeths1llBUO"
      },
      "source": [
        "<u>**Credits:**</u>\n",
        "- the setup work was done by [FutureArt](https://twitter.com/future__art) and [Pharmapsychotic](https://twitter.com/pharmapsychotic), credit goes to them for getting me started\n",
        "- I added the bulk of features useful to concept art and img2img\n",
        "- I used some public code to implement openCV facial recognition for face detailing\n",
        "- The workflow for the bonus **HD Generation** scripts was inspired by Inspired by [u/tokidokiyuki](https://www.reddit.com/user/tokidokiyuki/)\n",
        "[and their post on Reddit](https://www.reddit.com/r/StableDiffusion/comments/x45uk6/my_process_to_upscale_an_image_through_img2img/)\n",
        "- [uses Real ESRGAN from xinntao for upscaling](https://github.com/xinntao/Real-ESRGAN)\n",
        "\n",
        "<u>**Terms of Use:**</u>\n",
        "- This code is free to use however you want. You may use it, modify it and do anything with it without crediting me for the code (although it would be nice to get give me a mention if you use this code)\n",
        "- Any images you create using this code fall under the terms set by [huggingface.co](https://huggingface.co/CompVis/stable-diffusion) , [Stability.Ai](https://stability.ai/) as well as Google Colab terms. You must use this technology ethically. I am not responsible for anything you create, nor do I own any of your creations\n",
        "- I recommend only using input images you own the copywrite to, otherwise you may be liable to copywrite infringement \n",
        "- Any text prompts I share in code or elsewhere are free for you to use however you want, unrestricted and without credit to me. I believe prompts should be public and shared freely\n",
        "- You may use any of your creations privately and commercially without credit\n",
        "- Any images I create with this code are owned by me and you may not use any of my examples for commercial use, especially my original art. Respect me for sharing this code freely\n",
        "\n",
        "<br> \n",
        "\n",
        "---\n",
        "\n",
        "<u>**Key Features Include:**</u>\n",
        " - process a single image or multiple images, single prompts or multiple prompts\n",
        " - randomizing starting keyword in the prompts for variations on a theme\n",
        " - processing sequential guidance scales and image strengths on a seed for data\n",
        " - randomizing guidance scales and image strengths for exploration\n",
        " - processing a list of prompts with a corresponding image for consistency\n",
        " - processing a list of prompts with a random image for creativity\n",
        " - save prompt templates which can be quickly loaded\n",
        " - cycle through multiple prompt templates with multiple input images/prompts\n",
        " - the prompt cycling feature are like having your own art design team who can create different styles or follow a style!\n",
        " - sequence animation with ability to warp from after # of frames\n",
        " - face recognition to upscale the faces\n",
        " - HD upscaling by splitting the image into tiles\n",
        "\n",
        "<u>**Upcoming Plans (todo list):**</u>\n",
        "  - port locally to use with an RTX / better GUI (long term)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "<u>**Changelog:**</u>\n",
        "\n",
        "v 2.5.3, last updated 9/13/22\n",
        "- fixed some bugs that were introduced to the previous version. Added clarity on the # of prompts required for the grid upscaler\n",
        "\n",
        "v 2.5.2, last updated 9/13/22:\n",
        "- added a small script to copy images from / content/HD/preProcessedGridSplits to the Google Drive since it was a requested feature\n",
        "\n",
        "v 2.5.1, last updated 9/08/22:\n",
        "- added the grid preview to the grid upscaler bonus tool\n",
        "- fixed some bugs like the face recogntion was requiring a \"/\" in the filename, but it seems the copy/paste shortcut doesn't paste that /. So it was failing\n",
        "- discovered a memory leak, so after generating a lot of grid upscales, the REAL ESRGAN has a permanent CUDA out of memory error. Seems only solution is to restart runtime from the beginning\n",
        "\n",
        "v 2.5.0, last updated 9/08/22:\n",
        "- this is a special update for me because I fixed the issues with facial recognition that were present in the previous version. now it finally works as I envisioned!\n",
        "- I also moved the variables storing the prompt templates and batch prompting to the installation cells so they're auto created at start. They're now converted into text files which are easy to manage and update than the old cells system. This makes things much more user friendly. I might add some settings presents to text files as well, then it's less bloated than it currently is\n",
        "\n",
        "v 2.4.0, last updated 9/05/22:\n",
        "- added Real ESRGAN to use by itself for upscaling images, or for use with face recognition\n",
        "- got the face recognition to work and it will output files to Google Drive\n",
        "- the final results of face recognition are not auto blended into the base image. How to do this cleanly is beyond me, so I just make use of masking the result with the base\n",
        "- don't plan to add more code soon since there's a lot of features to work with\n",
        "\n",
        "v 2.3.0, last updated 9/04/22:\n",
        "- added the Bonus feature; HD Generation. Creates a generation for a 512x512 square across an input image. Inspired by [u/tokidokiyuki](https://www.reddit.com/user/tokidokiyuki/)\n",
        "[and their post on Reddit](https://www.reddit.com/r/StableDiffusion/comments/x45uk6/my_process_to_upscale_an_image_through_img2img/)\n",
        "\n",
        "v 2.2.2, last updated 9/02/22:\n",
        "- cleaned up the documentation, cleaned up the sections and reordered some fields\n",
        "- added aspect ratio templates\n",
        "\n",
        "v 1 to 2:  updated between 8/27/22 and 9/01/22:\n",
        "- experimented with core features to assist with concept art & design\n",
        "- added interpolation, multiple input images, prompt templates, etc\n",
        "- began exploring advanced features with openCV such as using facial recognition to clean up faces automatically\n",
        "\n",
        "v 1.0.0, last updated 8/27/22:\n",
        "- began the Stable Diffusion journey with FutureArt and Pharmapsychotic's collab file as a starting point\n",
        "- my initial intention was to take their version that had multiple prompts and allow those prompts to correspond with their own unique input images\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**How to download the Stable Diffusion model:**</u>\n",
        "\n",
        "- Visit https://huggingface.co/CompVis/stable-diffusion-v-1-4-original and agree to the terms and conditions.\n",
        "- Click the **Files and versions** tab\n",
        "- Click **stable-diffusion-v-1-4-original**\n",
        "- Click the **download** link where it says *This file is stored with Git LFS . It is too big to display, but you can still download it.*\n",
        "- If you have [Google Drive for desktop](https://www.google.com/drive/download/) (highly recommended), you can save it directly to your **AI/models** directory.\n",
        "  - Otherwise, download it and re-upload it to your [Google Drive](https://drive.google.com) in the **AI/models** directory. (This is risky, as the upload may time out.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GOr30k9Gh1L"
      },
      "source": [
        "# 1. <u> Setup (Run All at Start) </u>\n",
        "- simply run the arrow here to set everything up, no need to open this section or mess with settings (time to set up: 3-5 mins)\n",
        "- make sure you have the Stable Diffusion model saved to your drive in AI/models/sd-v1-4.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4920437-2274-4e1b-9fdf-feb55fb41307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1a175b88-ca83-a663-7015-6174a84f956f)\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c52bcd8-3bf5-496a-cc1e-05d0168bd72d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Outputs will be saved to /content/gdrive/MyDrive/AI/StableDiffusionBatch\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusionBatch\"\n",
        "!mkdir -p $outputs_path\n",
        "!mkdir -p \"init_image\"\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7KPSsU8zhut"
      },
      "source": [
        "### Prompt Templates and Prompt Variables Setup\n",
        "- If you wish to save prompt templates for future session, be sure to write them here!\n",
        "- All the data in these variables will be saved to txt files in / content/promptTexts\n",
        "- Reloading this colab later will overwrite those text files with the default settings here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbWL5peWrzMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad97343d-8ae1-4584-a1ed-7b538aa4326c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfilename =\"/content/promptTexts/promptTemplateDictionary.txt\"\\nwith open(filename, \\'w\\') as f: \\n    for key, value in promptTemplateDict.items(): \\n        f.write(\\'%s:%s\\n\\' % (key, value))\\nwith open(filename) as f_input:\\n    data = f_input.read().rstrip(\\'\\n\\')\\nwith open(filename, \\'w\\') as f_output:    \\n    f_output.write(data)\\n\\nfilename =\"/content/promptTexts/promptInterpolateMulti.txt\"\\nwith open(filename, \\'w\\') as f: \\n    for key, value in promptInterpolateMulti.items(): \\n        f.write(\\'%s:%s\\n\\' % (key, value))\\nwith open(filename) as f_input:\\n    data = f_input.read().rstrip(\\'\\n\\')\\nwith open(filename, \\'w\\') as f_output:    \\n    f_output.write(data)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Note: this data is essential for running prompt templates. Running this will save initially to text file to make editing easier\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "batch_idx = 0 # some users were getting errors for this and the number can get bloated after many generations, so this is a temp solution to avoid issues with it\n",
        "# CELL 1: PROMPT TEMPLATE (run)\n",
        "\n",
        "# Here you can save prompt templates for reusing:\n",
        "# if you want to edit or add your own to the template, make sure to respect Python dict: key: ['templateStartWord', 'prompt details'],\n",
        "# in addition to writing the prompts here, you need to add them to the promptTemplate #@param in the main code, otherwise it won't show up to select\n",
        "promptTemplateDict = {\n",
        "0: ['', 'none'],\n",
        "1: ['female', 'portrait, young woman, detailed gorgeous face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha'],\n",
        "2: ['male', 'portrait, mature man, detailed rugged face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by J. C. Leyendecker and greg rutkowski and Norman Rockwell'],\n",
        "3: ['science fiction', 'scifi, robotics, fururistic decor, trending on artstation, global illumination, matte painting, high detail, unreal engine 5, halo, star citizen, star wars, by sparth, octane render, by ralph McQuarrie, by Syd Mead, by ryan church'],\n",
        "4: ['fantasy', 'painterly, nature, natural, guild wars, magical, ethereal, highly detailed, historical, period piece, by greg rutkowski, by Daniel Dociu, by noah bradley, by howard lyon'],\n",
        "5: ['lineart drawing', 'drawn by bernie wrightson, industrial design sketch, sketch by Scott Robertson'],\n",
        "6: ['Redshift render', 'sharp, rendered in octane, highly detailed, minimalistic, rendered in unreal engine 5, product render'],\n",
        "7: ['hard surface, weapon design, scifi gun, halo, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by greg rutkowski'],\n",
        "8: ['a wholesome animation key shot of', 'studio ghibli, pixar and disney animation, sharp, rendered in unreal engine 5, anime key art by greg rutkowski, bloom, dramatic lighting'],\n",
        "9: ['a scene of', 'black and white, comic book art, by yoji shinkawa and takehiko inoue and kim jung gi, masterpiece, perfect'],\n",
        "10: ['intricate fine tipped pen drawing of a', 'inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey'],\n",
        "11: ['full body 3d render of', 'as a vinyl action figure, studio lighting, white background, blender, trending on artstation, 8k, highly detailed, rendered in redshift, studio product photograph'],\n",
        "12: ['sticker of', 'cute sticker decal design, highly detailed, high quality, digital painting'],\n",
        "13: ['marble statue', 'sculpture, marble, ancient greek statue, museum statue from stone, stone sculpt'],\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#CELL 2a: INTERPOLATION PROMPTS (run this cell & the next one)\n",
        "# use this section to change the prompt being interpolated over time (frames held by prompt)\n",
        "# More Information:\n",
        "# key#: [frameCount, promptString]\n",
        "# at 30% playback speed of 30FPS video, #10 frames = 1 second\n",
        "# best frame length 0.2: initial prompt = chaneg on 8 or 13 (5 increments); after change on 5,10,15, etc (this will change prompt as soon as we land back on 0.6, granted we go down to 0.2)\n",
        "# best frame length 0.3: initial prompt = change on 7 or 11 (4 increments); after change on 4,8,12, etc\n",
        "\n",
        "promptInterpolateMulti = {\n",
        "0: [10, 'a frog relaxing on a lilypad while ribbiting'],\n",
        "1: [10, 'a frog transforming into a cat, white background'],\n",
        "2: [10, 'a cat hissing and growling in a creepy forest, white background'],\n",
        "3: [10, 'a cat growing into a massive lion, white background'],\n",
        "4: [10, 'a lion roaring, white background'],\n",
        "5: [20, 'a exploding, explosion, burst of flames'],\n",
        "6: [10, 'a face of a beautiful woman turning into a flower'],\n",
        "7: [30, 'flower pedals transforming into a swarm of butterflies'],\n",
        "8: [20, 'flower pedals transforming into a swarm of butterflies'],\n",
        "9: [20, 'butterflies shrinking into tiny stars in a starry night'],\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "##@title # CELL 2b: MAIN PROMPTS & INITIAL IMAGES (run)\n",
        "#Info = 'click Show Code to view, click Run Cell to update:' #@param [\"click Show Code to view, click Run Cell to update:\"]\n",
        "# IMPORTANT 1: Make sure to update this cell every time you change prompts and init_images!\n",
        "# IMPORTANT 2: For the prompts and images to match, make sure to sync their position in the list\n",
        "\n",
        "# Put each of your prompts on a new line:\n",
        "promptsA = '''\n",
        "a happy rabbit munching on broccoli\n",
        "Nyx, goddess of the moon and night\n",
        "'''\n",
        "\n",
        "\n",
        "# In order to use the dictionaries in a user friendly way and load with Python, I had to reformat the variables to be text rather than dictionary:\n",
        "promptTemplateDict = '''\n",
        "female|portrait, young woman, detailed gorgeous face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha\n",
        "male|portrait, mature man, detailed rugged face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by J. C. Leyendecker and greg rutkowski and Norman Rockwell\n",
        "science fiction|scifi, robotics, fururistic decor, trending on artstation, global illumination, matte painting, high detail, unreal engine 5, halo, star citizen, star wars, by sparth, octane render, by ralph McQuarrie, by Syd Mead, by ryan church\n",
        "fantasy|painterly, nature, natural, guild wars, magical, ethereal, highly detailed, historical, period piece, by greg rutkowski, by Daniel Dociu, by noah bradley, by howard lyon\n",
        "lineart drawing|drawn by bernie wrightson, industrial design sketch, sketch by Scott Robertson\n",
        "Redshift render|sharp, rendered in octane, highly detailed, minimalistic, rendered in unreal engine 5, product render\n",
        "hard surface weapon design|scifi gun, halo, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by greg rutkowski\n",
        "a wholesome animation key shot of|studio ghibli, pixar and disney animation, sharp, rendered in unreal engine 5, anime key art by greg rutkowski, bloom, dramatic lighting\n",
        "a scene of|black and white, comic book art, by yoji shinkawa and takehiko inoue and kim jung gi, masterpiece, perfect']\n",
        "intricate fine tipped pen drawing of a|inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey\n",
        "full body 3d render of|as a vinyl action figure, studio lighting, white background, blender, trending on artstation, 8k, highly detailed, rendered in redshift, studio product photograph\n",
        "sticker of|cute sticker decal design, highly detailed, high quality, digital painting\n",
        "marble statue|sculpture, marble, ancient greek statue, museum statue from stone, stone sculpt\n",
        "'''\n",
        "\n",
        "promptInterpolateMulti = '''\n",
        "10|a frog relaxing on a lilypad while ribbiting\n",
        "10|a frog transforming into a cat, white background\n",
        "10|a cat hissing and growling in a creepy forest, white background\n",
        "10|a cat growing into a massive lion, white background\n",
        "10|a lion roaring, white background\n",
        "20|a exploding, explosion, burst of flames\n",
        "10|a face of a beautiful woman turning into a flower\n",
        "30|flower pedals transforming into a swarm of butterflies\n",
        "20|flower pedals transforming into a swarm of butterflies\n",
        "20|butterflies shrinking into tiny stars in a starry night\n",
        "'''\n",
        "\n",
        "promptEmotiveTemplate = '''\n",
        "happy|happy friendly cheerful smiling joy sparkling magical\n",
        "angry|villain angry pissed off frowning yelling spitting fighting violent extreme anger rage fury outrage furious open mouth\n",
        "sad|depressed sad saddened heartbroken pouting crying tears frown grief grieving hopeless whimpering\n",
        "zombie|black dark very dark black and creepy horror terrifying bloody gore wounds blood splats zombie terrified dead\n",
        "psychedelic|psychedelic trippy colorful neon kaleidoscope mushrooms trance bright mind-blowing mystical spiritual mysticism psilocybin hallucinogenic\n",
        "sketchy drawing style|line art line drawing pen and ink black and white simple simplified sketch drawing pencil drawing rough thin lines line linear etched flat drawing\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# code portion:\n",
        "if os.path.exists('/content/promptTexts') == False:\n",
        "  os.mkdir(\"/content/promptTexts/\")\n",
        "\n",
        "fle = Path('/content/promptTexts/_promptsMain.txt')\n",
        "fle.touch(exist_ok=True)\n",
        "f = open(fle)\n",
        "fle = Path('/content/promptTexts/promptTemplateDictionary.txt')\n",
        "fle.touch(exist_ok=True)\n",
        "f = open(fle)\n",
        "fle = Path('/content/promptTexts/promptInterpolateMulti.txt')\n",
        "fle.touch(exist_ok=True)\n",
        "f = open(fle)\n",
        "fle = Path('/content/promptTexts/promptEmotiveTemplate.txt')\n",
        "fle.touch(exist_ok=True)\n",
        "f = open(fle)\n",
        "\n",
        "filename =\"/content/promptTexts/_promptsMain.txt\"\n",
        "with open(filename, 'w') as output:\n",
        "    promptsA = [i for i in promptsA.split('\\n') if i]\n",
        "    for item in promptsA:\n",
        "        # write each item on a new line\n",
        "        output.write(\"%s\\n\" % item)\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "\n",
        "filename =\"/content/promptTexts/promptInterpolateMulti.txt\"\n",
        "with open(filename, 'w') as output:\n",
        "    promptInterpolateMulti = [i for i in promptInterpolateMulti.split('\\n') if i]\n",
        "    for item in promptInterpolateMulti:\n",
        "        # write each item on a new line\n",
        "        output.write(\"%s\\n\" % item)\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "\n",
        "filename =\"/content/promptTexts/promptTemplateDictionary.txt\"\n",
        "with open(filename, 'w') as output:\n",
        "    promptTemplateDict = [i for i in promptTemplateDict.split('\\n') if i]\n",
        "    for item in promptTemplateDict:\n",
        "        # write each item on a new line\n",
        "        output.write(\"%s\\n\" % item)\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "\n",
        "filename =\"/content/promptTexts/promptEmotiveTemplate.txt\"\n",
        "with open(filename, 'w') as output:\n",
        "    promptEmotiveTemplate = [i for i in promptEmotiveTemplate.split('\\n') if i]\n",
        "    for item in promptEmotiveTemplate:\n",
        "        # write each item on a new line\n",
        "        output.write(\"%s\\n\" % item)\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "\n",
        "\n",
        "# this code is now defunct since we're converting the dictionaries to regular text instead and turning into dict when they are loaded in python:\n",
        "'''\n",
        "filename =\"/content/promptTexts/promptTemplateDictionary.txt\"\n",
        "with open(filename, 'w') as f: \n",
        "    for key, value in promptTemplateDict.items(): \n",
        "        f.write('%s:%s\\n' % (key, value))\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "\n",
        "filename =\"/content/promptTexts/promptInterpolateMulti.txt\"\n",
        "with open(filename, 'w') as f: \n",
        "    for key, value in promptInterpolateMulti.items(): \n",
        "        f.write('%s:%s\\n' % (key, value))\n",
        "with open(filename) as f_input:\n",
        "    data = f_input.read().rstrip('\\n')\n",
        "with open(filename, 'w') as f_output:    \n",
        "    f_output.write(data)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnavTMQkkUO-"
      },
      "source": [
        "### Lengthy Installations & Prep to Backend for Stable Diffusion\n",
        "- these cells take the longest to load\n",
        "- The define helper functions will interact with the variables we set up throughout our code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XA1NNcxM724U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6fa6f6-ef7f-4be2-fefe-df358a3f7282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.7.5-py3-none-any.whl (706 kB)\n",
            "\u001b[K     |████████████████████████████████| 706 kB 15.1 MB/s \n",
            "\u001b[?25hCollecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Collecting tensorboard>=2.9.1\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 83.5 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.48.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from torch-fidelity) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from torch-fidelity) (0.13.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-fidelity) (1.7.3)\n",
            "Installing collected packages: torchmetrics, tensorboard, pyDeprecate, torch-fidelity, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pyDeprecate-0.3.2 pytorch-lightning-1.7.5 tensorboard-2.10.0 torch-fidelity-0.3.0 torchmetrics-0.9.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "\u001b[K     |████████████████████████████████| 565 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.7.5)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 91.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.1.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.8.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.48.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning) (3.2.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=bfeeca5f9cb5ffb811daa0dacfd826f1772dbdd7f3a6825e847c00d46fac0dbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, kornia, einops\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 einops-0.4.1 kornia-0.6.7 omegaconf-2.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (6.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.7.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.6.0.66)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (4.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting jsonmerge\n",
            "  Downloading jsonmerge-1.8.0.tar.gz (26 kB)\n",
            "Collecting resize-right\n",
            "  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from jsonmerge) (4.3.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->jsonmerge) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->jsonmerge) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->jsonmerge) (5.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->jsonmerge) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->jsonmerge) (3.8.1)\n",
            "Building wheels for collected packages: jsonmerge\n",
            "  Building wheel for jsonmerge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonmerge: filename=jsonmerge-1.8.0-py3-none-any.whl size=18013 sha256=5d69c6a88621321ae57fd09900531da5f83da68389397798a539bd531b21c105\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c8/79/83ddc70e0b20f2df3bbac658c2c5d665b76cedd02e67bd61dc\n",
            "Successfully built jsonmerge\n",
            "Installing collected packages: torchdiffeq, resize-right, jsonmerge, ftfy\n",
            "Successfully installed ftfy-6.1.1 jsonmerge-1.8.0 resize-right-0.0.2 torchdiffeq-0.2.3\n",
            "Cloning into 'stable-diffusion'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Total 313 (delta 0), reused 0 (delta 0), pack-reused 313\u001b[K\n",
            "Receiving objects: 100% (313/313), 42.62 MiB | 11.12 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "/content/stable-diffusion\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1335, done.\u001b[K\n",
            "remote: Total 1335 (delta 0), reused 0 (delta 0), pack-reused 1335\n",
            "Receiving objects: 100% (1335/1335), 409.77 MiB | 26.14 MiB/s, done.\n",
            "Resolving deltas: 100% (279/279), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 236 (delta 5), reused 7 (delta 3), pack-reused 221\u001b[K\n",
            "Receiving objects: 100% (236/236), 8.92 MiB | 5.54 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n",
            "Cloning into 'k-diffusion'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 531 (delta 124), reused 121 (delta 89), pack-reused 363\u001b[K\n",
            "Receiving objects: 100% (531/531), 108.32 KiB | 9.03 MiB/s, done.\n",
            "Resolving deltas: 100% (353/353), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n",
        "\n",
        "# Installation Setup for Facial Recognition when using Crop Upres Feature:\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mTjVEfsNlDly"
      },
      "outputs": [],
      "source": [
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "669338669e324aff9b60eae686382049",
            "663964ff6cd6450eb0f63c11b8b88b02",
            "ad2b4c004e30431c8c170ac4785711ba",
            "bba8f8f59d8741f997f9678fde15c6f2",
            "e61b6c43495c4eb9bf8f67e92f554944",
            "1ceb6a49daa8452ba59e48d173bdf85d",
            "3f0219a3018f4c7c87c91ca2ea174c28",
            "5ffbe7a80a4b408aaa8ca665f4279994",
            "e4cc53220559474ebec5409a3a8821c7",
            "dddb8c3e243b4a4da5bb0f4db1a93e9f",
            "5cfc9fcada99451cbef6f8c3a8deebc0",
            "2f423adc48b84d29b71e7a77fc2624bf",
            "68e431306ccb484685ba01dcd3ed553f",
            "9662f319d8ef498a95bb29409256fcd6",
            "3f340c3fe04d4699a911489a2ff26c24",
            "5a50671e27c54d46bf64f3af0dfbca86",
            "aa6ff2f87e824a148d106ee9c860d253",
            "9e12c27425604668a71fbfe24a07e115",
            "36347058c5e0418c8d3333e4f06a6f3f",
            "889b93649edd4802a92be13c69615997",
            "351251aae3674fa4a98795c45e793f74",
            "dd7bcc25d9fd4da3b1bffdc5cc772e1e",
            "8071b9d736f64a39b2728981f21a232f",
            "162a956fca16418bbc30ea43de7725a0",
            "1e8bfe57bd274bd0821beabe1717e5c1",
            "f13bd986b7144799b4cb8aaf9230a547",
            "fc27fc3e511844ba82057516811a2a28",
            "f8a5eecd365c417f85d9da8e886fdc86",
            "f97f7f85350b49b6a0a059b86322b7a6",
            "3973866f8c3a44dfaea0a17eddbbde76",
            "3772a7d47420441bbc1b311c42590142",
            "c7bf2f1905b349089a2908e0bf6179e9",
            "6a01e1743e194efaa0cb5ce5de93c045",
            "9beda51032b5432fbe846ac121b425cf",
            "166bd2a1b47445aca1e7656b3d1dcd19",
            "bf616f2afeb8401fbd25ee7bc40d0ff1",
            "449fc373eca54ea185c5de9737bdfb3c",
            "1bdfdd5f7f814f818cb8ae15a99646d2",
            "1afd5a2727a041d79d4e4593b316db41",
            "a49b55f740c54c668bf6e2f6a83f1e72",
            "01a9e781f46443af857f60af0f1a2cb0",
            "70f45ca57f2546c6a702d9fb1b37a168",
            "b44424773f904770b4d9751e66609178",
            "7c446cdf78eb430dbea34e500bd29f3d",
            "eedf09703b674a879a220c4ee09806ff",
            "724a8d58b76945e6a2d3c8c720c1ed4c",
            "18ec5a7e730046fca9d8e98560efa18b",
            "bda1fce5aa9444faa1d87967fbddb8b3",
            "03e6abc874c34d4390eb6f8623771c5f",
            "b2fcd78b605d4ef4b9598d9194923e15",
            "59f3b82cf21042179e95181bab4220b9",
            "629168bf5ecb42deb0921e14213c9ef6",
            "5886a1a01a7e4fb6b993aafcdabb0a68",
            "8645c4941bf34aaa8a1e487bc14bf235",
            "00fa9bee8da34747abb255cb0f3b52ad",
            "51abe0445d7d407898bc713fcdae648c",
            "a3e0bf22f57c4382989b63c3c5dda941",
            "4edb5d612a64451ca2fe9202f3d46c4e",
            "64ff97d93f834346ba374ac0e3a3603a",
            "cd957ff1cdae42e6a07c48b21a99a1a5",
            "0cdcbcee5b9549a4a2f30c0df8802aa6",
            "e1a4d9fbf5e2453889487632c4992e04",
            "b1b1074628004a5ab120b1158c0ddb4c",
            "abbaa6f5ee6c4184b7b983ca1d7c0155",
            "fef1cf3b3ad443dabd7504c5ab546ac2",
            "59694b308c28462a93fea3c7a3c33b98"
          ]
        },
        "id": "bcHsbr3hblrk",
        "outputId": "5258b885-3b15-44fd-b2ed-f279b8aa955e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/939k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "669338669e324aff9b60eae686382049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/512k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f423adc48b84d29b71e7a77fc2624bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8071b9d736f64a39b2728981f21a232f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9beda51032b5432fbe846ac121b425cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eedf09703b674a879a220c4ee09806ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51abe0445d7d407898bc713fcdae648c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#@title Define helper functions\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch.cuda.amp import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "#from init_image.dmdim import DDIMSampler # customer sampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'full' # 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, w, h):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize((w, h), Image.LANCZOS)\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    images = []\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler == 'klms':\n",
        "                                print(\"Using KLMS sampling\")\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            try:\n",
        "                              if opt.saveNumberPosition == False: # don't save the number at beginning of the file\n",
        "                                filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx:03})_{sample_idx:04} {opt.seed}.png\")#####\n",
        "                              else: # put the number at the beginning of the filename\n",
        "                                filepath = os.path.join(opt.outdir, f\"{sample_idx:04} {batch_name}({batch_idx:03}) {opt.seed}.png\")#####\n",
        "                            except:\n",
        "                                filepath = os.path.join(opt.outdir, f\"{sample_idx:04} {batch_name}({batch_idx:03}) {opt.seed}.png\")#####                              \n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86Mo8iYbhQuF",
        "outputId": "4eb30208-16e6-43d9-d2d1-14a5ecbcb566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Real-ESRGAN'...\n",
            "remote: Enumerating objects: 693, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 693 (delta 7), reused 17 (delta 1), pack-reused 665\u001b[K\n",
            "Receiving objects: 100% (693/693), 5.36 MiB | 24.20 MiB/s, done.\n",
            "Resolving deltas: 100% (362/362), done.\n",
            "/content/stable-diffusion/Real-ESRGAN\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting basicsr\n",
            "  Downloading basicsr-1.4.2.tar.gz (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 14.8 MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.16.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from basicsr) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from basicsr) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.7.3)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.11.0a20220913-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 81.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.64.1)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 83.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->basicsr) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2.10)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2021.11.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.48.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->basicsr) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->basicsr) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (3.2.0)\n",
            "Building wheels for collected packages: basicsr\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214840 sha256=45fefc06005797fc322a2d2572167186f01cae8e4d111bd793cd26898f0843a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/b3/4a/e2bc545f98417b6766ca50dd82b2a1f2b37780c68d41da9ca9\n",
            "Successfully built basicsr\n",
            "Installing collected packages: yapf, tb-nightly, addict, basicsr\n",
            "Successfully installed addict-2.4.0 basicsr-1.4.2 tb-nightly-2.11.0a20220913 yapf-0.32.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting facexlib\n",
            "  Downloading facexlib-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from facexlib) (4.64.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from facexlib) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.7.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib) (0.56.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.12.1+cu113)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from facexlib) (4.6.0.66)\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 30.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facexlib) (0.13.1+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy->facexlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (0.39.1)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->facexlib) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->facexlib) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (3.0.4)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=4fb8cc85edc3c08de1f251167ef9bad04e1a411b48751c55010f7a58edccc21c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy, facexlib\n",
            "Successfully installed facexlib-0.2.5 filterpy-1.4.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gfpgan\n",
            "  Downloading gfpgan-1.3.7-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gfpgan) (4.64.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.99)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.21.6)\n",
            "Requirement already satisfied: facexlib>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.2.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gfpgan) (4.6.0.66)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.7/dist-packages (from gfpgan) (2.11.0a20220913)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.32.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.7.3)\n",
            "Requirement already satisfied: basicsr>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.4.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.13.1+cu113)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gfpgan) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (0.18.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (0.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (7.1.2)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->gfpgan) (1.4.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->gfpgan) (0.56.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->gfpgan) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib>=0.2.5->gfpgan) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.15.0)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->facexlib>=0.2.5->gfpgan) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.48.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->gfpgan) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->gfpgan) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->gfpgan) (3.2.0)\n",
            "Installing collected packages: gfpgan\n",
            "Successfully installed gfpgan-1.3.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: basicsr>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: facexlib>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.2.5)\n",
            "Requirement already satisfied: gfpgan>=1.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (4.64.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.99)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.32.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.11.0a20220913)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->-r requirements.txt (line 2)) (0.56.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->-r requirements.txt (line 2)) (0.39.1)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->-r requirements.txt (line 2)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->facexlib>=0.2.5->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (2021.11.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.48.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "running develop\n",
            "running egg_info\n",
            "creating realesrgan.egg-info\n",
            "writing realesrgan.egg-info/PKG-INFO\n",
            "writing dependency_links to realesrgan.egg-info/dependency_links.txt\n",
            "writing requirements to realesrgan.egg-info/requires.txt\n",
            "writing top-level names to realesrgan.egg-info/top_level.txt\n",
            "writing manifest file 'realesrgan.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'realesrgan.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/dist-packages/realesrgan.egg-link (link to .)\n",
            "Adding realesrgan 0.2.5.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/stable-diffusion/Real-ESRGAN\n",
            "Processing dependencies for realesrgan==0.2.5.0\n",
            "Searching for tqdm==4.64.1\n",
            "Best match: tqdm 4.64.1\n",
            "Adding tqdm 4.64.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torchvision==0.13.1+cu113\n",
            "Best match: torchvision 0.13.1+cu113\n",
            "Adding torchvision 0.13.1+cu113 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.12.1+cu113\n",
            "Best match: torch 1.12.1+cu113\n",
            "Adding torch 1.12.1+cu113 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Pillow==7.1.2\n",
            "Best match: Pillow 7.1.2\n",
            "Adding Pillow 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for opencv-python==4.6.0.66\n",
            "Best match: opencv-python 4.6.0.66\n",
            "Adding opencv-python 4.6.0.66 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.21.6\n",
            "Best match: numpy 1.21.6\n",
            "Adding numpy 1.21.6 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for gfpgan==1.3.7\n",
            "Best match: gfpgan 1.3.7\n",
            "Adding gfpgan 1.3.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for facexlib==0.2.5\n",
            "Best match: facexlib 0.2.5\n",
            "Adding facexlib 0.2.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for basicsr==1.4.2\n",
            "Best match: basicsr 1.4.2\n",
            "Adding basicsr 1.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==4.1.1\n",
            "Best match: typing-extensions 4.1.1\n",
            "Adding typing-extensions 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyYAML==6.0\n",
            "Best match: PyYAML 6.0\n",
            "Adding PyYAML 6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for lmdb==0.99\n",
            "Best match: lmdb 0.99\n",
            "Adding lmdb 0.99 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.7.3\n",
            "Best match: scipy 1.7.3\n",
            "Adding scipy 1.7.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for yapf==0.32.0\n",
            "Best match: yapf 0.32.0\n",
            "Adding yapf 0.32.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "Installing yapf-diff script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tb-nightly==2.11.0a20220913\n",
            "Best match: tb-nightly 2.11.0a20220913\n",
            "Adding tb-nightly 2.11.0a20220913 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for filterpy==1.4.5\n",
            "Best match: filterpy 1.4.5\n",
            "Adding filterpy 1.4.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numba==0.56.2\n",
            "Best match: numba 0.56.2\n",
            "Adding numba 0.56.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for future==0.16.0\n",
            "Best match: future 0.16.0\n",
            "Adding future 0.16.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scikit-image==0.18.3\n",
            "Best match: scikit-image 0.18.3\n",
            "Adding scikit-image 0.18.3 to easy-install.pth file\n",
            "Installing skivi script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for addict==2.4.0\n",
            "Best match: addict 2.4.0\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2022.6.15\n",
            "Best match: certifi 2022.6.15\n",
            "Adding certifi 2022.6.15 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for wheel==0.37.1\n",
            "Best match: wheel 0.37.1\n",
            "Adding wheel 0.37.1 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.6\n",
            "Best match: google-auth-oauthlib 0.4.6\n",
            "Adding google-auth-oauthlib 0.4.6 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth==1.35.0\n",
            "Best match: google-auth 1.35.0\n",
            "Adding google-auth 1.35.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-data-server==0.6.1\n",
            "Best match: tensorboard-data-server 0.6.1\n",
            "Adding tensorboard-data-server 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Markdown==3.4.1\n",
            "Best match: Markdown 3.4.1\n",
            "Adding Markdown 3.4.1 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for protobuf==3.17.3\n",
            "Best match: protobuf 3.17.3\n",
            "Adding protobuf 3.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==57.4.0\n",
            "Best match: setuptools 57.4.0\n",
            "Adding setuptools 57.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.8.1\n",
            "Best match: tensorboard-plugin-wit 1.8.1\n",
            "Adding tensorboard-plugin-wit 1.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for grpcio==1.48.1\n",
            "Best match: grpcio 1.48.1\n",
            "Adding grpcio 1.48.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for absl-py==1.2.0\n",
            "Best match: absl-py 1.2.0\n",
            "Adding absl-py 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for importlib-metadata==4.12.0\n",
            "Best match: importlib-metadata 4.12.0\n",
            "Adding importlib-metadata 4.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for llvmlite==0.39.1\n",
            "Best match: llvmlite 0.39.1\n",
            "Adding llvmlite 0.39.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for imageio==2.9.0\n",
            "Best match: imageio 2.9.0\n",
            "Adding imageio 2.9.0 to easy-install.pth file\n",
            "Installing imageio_download_bin script to /usr/local/bin\n",
            "Installing imageio_remove_bin script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyWavelets==1.3.0\n",
            "Best match: PyWavelets 1.3.0\n",
            "Adding PyWavelets 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tifffile==2021.11.2\n",
            "Best match: tifffile 2021.11.2\n",
            "Adding tifffile 2021.11.2 to easy-install.pth file\n",
            "Installing lsm2bin script to /usr/local/bin\n",
            "Installing tiff2fsspec script to /usr/local/bin\n",
            "Installing tiffcomment script to /usr/local/bin\n",
            "Installing tifffile script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for networkx==2.6.3\n",
            "Best match: networkx 2.6.3\n",
            "Adding networkx 2.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests-oauthlib==1.3.1\n",
            "Best match: requests-oauthlib 1.3.1\n",
            "Adding requests-oauthlib 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cachetools==4.2.4\n",
            "Best match: cachetools 4.2.4\n",
            "Adding cachetools 4.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for rsa==4.9\n",
            "Best match: rsa 4.9\n",
            "Adding rsa 4.9 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for kiwisolver==1.4.4\n",
            "Best match: kiwisolver 1.4.4\n",
            "Adding kiwisolver 1.4.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cycler==0.11.0\n",
            "Best match: cycler 0.11.0\n",
            "Adding cycler 0.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyparsing==3.0.9\n",
            "Best match: pyparsing 3.0.9\n",
            "Adding pyparsing 3.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for zipp==3.8.1\n",
            "Best match: zipp 3.8.1\n",
            "Adding zipp 3.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for oauthlib==3.2.0\n",
            "Best match: oauthlib 3.2.0\n",
            "Adding oauthlib 3.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for realesrgan==0.2.5.0\n",
            "--2022-09-13 15:56:58--  https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/387326890/08f0e941-ebb7-48f0-9d6a-73e87b710e7e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220913T155544Z&X-Amz-Expires=300&X-Amz-Signature=610ece0912199f209d4d64e67fae0ac35b56fb22500e469d4713f6c7127e021e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=387326890&response-content-disposition=attachment%3B%20filename%3DRealESRGAN_x4plus.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-13 15:56:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/387326890/08f0e941-ebb7-48f0-9d6a-73e87b710e7e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220913T155544Z&X-Amz-Expires=300&X-Amz-Signature=610ece0912199f209d4d64e67fae0ac35b56fb22500e469d4713f6c7127e021e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=387326890&response-content-disposition=attachment%3B%20filename%3DRealESRGAN_x4plus.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67040989 (64M) [application/octet-stream]\n",
            "Saving to: ‘experiments/pretrained_models/RealESRGAN_x4plus.pth’\n",
            "\n",
            "RealESRGAN_x4plus.p 100%[===================>]  63.93M  28.1MB/s    in 2.3s    \n",
            "\n",
            "2022-09-13 15:57:01 (28.1 MB/s) - ‘experiments/pretrained_models/RealESRGAN_x4plus.pth’ saved [67040989/67040989]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Run this code to install Real-ESRGAN\n",
        "# this is a requirement for the bonus tools - face recognition\n",
        "\n",
        "#Before start, make sure that you choose:\n",
        "#* Runtime Type = Python 3\n",
        "#* Hardware Accelerator = GPU\n",
        "#in the **Runtime** menu -> **Change runtime type**\n",
        "#Then, we clone the repository, set up the envrironment, and download the pre-trained model.\n",
        "\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "# Download the pre-trained model\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGldtYOMb5fk"
      },
      "source": [
        "# <u> Information on How to Use</u> *(DON'T RUN; this is Purely Informative / Documentation)*\n",
        "- this section only contains reminders and tips on usage. Section 2. Batch Prompting is the working code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-gyim7jyfJ76"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#@title Prompting:\n",
        "#@markdown `NOTE: The settings in this informative cell are the code defaults for the most basic use.` \n",
        "<br>\n",
        "#@markdown `If you have issues or want to revert, set the fields in the code cell (inside Section 2. Batch Prompting) to these values`\n",
        "<br>\n",
        "#@markdown `It's also best to make sure all the optional fields are set to 0, checked off, or none unless you intend to use the feature`\n",
        "\n",
        "\n",
        "##1---------------------------------------------------------------------------------------------\n",
        "#@markdown ---\n",
        "#@markdown > <u>**Section 1): Saving/Folder Settings** *(required):* <br></u>\n",
        "##@markdown Shared Prompt Ending:\n",
        "#@markdown 1b) **`outputFolder`**\n",
        "#@markdown - folder name where this session will store the images<br>\n",
        "#@markdown - This folder will exist (by default) under content/gdrive/MyDrive/AI/StableDiffusionBatch/`\n",
        "<br>\n",
        "#@markdown 1b) **`saveSeperatePromptsToSubfolders`**\n",
        "#@markdown - True = save each prompt in a seperate folder, first 3 strings in prompt = folder name\n",
        "#@markdown - False = save all prompts results in the same outputFolder (defined above in 1a)<br>\n",
        "<br>\n",
        "1a) outputFolder = \"upres_portraits4\" #@param {type:\"string\"}\n",
        "1b) saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##2---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 2): Prompt Template** *(optional):*</u>\n",
        "<br>\n",
        "#@markdown `Note: set to 'none/custom' if you rather type your prompt details than use a preset. This is for convenience`\n",
        "<br>\n",
        "#@markdown `Note: adding a new preset requires these 3 steps: `\n",
        "#@markdown - 1) adding a new line to the promptTemplateDict dictionary (number it correctly too)\n",
        "#@markdown - 2) adding this prompt as a new drop down option in promptTemplate (in the cell code, line 31~)\n",
        "#@markdown - 3) adding the dropdown text as an additional elif statement in the code (look at the pattern and repeat)\n",
        "<br>\n",
        "#@markdown 2a) **`promptTemplate`**\n",
        "#@markdown - the core prompt templates. You can select a template to reuse the same starting word + ending strings used in the prompt\n",
        "#@markdown - use these to save the inconvenience of copy/pasting the same text over and over again. focus on writing the subject instead\n",
        "#@markdown - *Python structure -* **key: ['templateStartWord', 'prompt details']** | example - 10: ['tattoo design of', 'inking, drawing, detailed, art by artist']\n",
        "\n",
        "# in addition to writing the prompts here, you need to add them to the promptTemplate #@param in the main code, otherwise it won't show up to select\n",
        "\n",
        "promptTemplateDict = {\n",
        "0: ['', 'none'],\n",
        "<br>\n",
        "#@markdown 2b) **`emotiveTemplate`**\n",
        "#@markdown - similar to promptTemplate, these strings only go at the end of the prompt to give a consistent influence to results\n",
        "#@markdown - example of use: if you want variants of all your character portraits to be zombies, or to be happy, use this feature\n",
        "#@markdown - *disclaimer:* may not work as intended all the time (especially with long prompts; this text gets truncated\n",
        "<br>\n",
        "#@markdown 2c) **`randomEnd`**\n",
        "#@markdown - text that goes at the end of all prompts. use comma to split the text and randomize between comma\n",
        "<br>\n",
        "\n",
        "2a) promptTemplate = 'none/custom' #@param [\"none/custom\",\"character - female adult\", \"character - male adult\", \"environment - scifi\", \"environment - fantasy\", \"vehicle - sketch\", \"vehicle - render\", \"style - anime keyframe (use high guidance scale of 15+)\", \"style - comic art inks\", \"style - blackwork / heavily inked tattoos\", \"fun - action figure character\", \"fun - stickers\", \"fun - marble statue\"]\n",
        "2b) emotiveTemplate = 'none' #@param [\"none\", \"happy\", \"angry\", \"sad\", \"zombie\", \"psychedelic\", \"sketchy drawing style\"]\n",
        "2c) randomEnd = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##3---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 3): Prompt Details** *(optional - toggle on/off):* <br></u>\n",
        "<br>\n",
        "#@markdown `Note: Your prompt must be 77 tokens or less. Any longer and the extra text gets truncated` <br>\n",
        "#<br>\n",
        "#@markdown There's many options for splitting up the prompts:<br>\n",
        "#<br>\n",
        "#@markdown - 3a): **`useThese3PromptFields`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\n",
        "#<br>\n",
        "#@markdown  - 3b): **`randomStart`:** an initial word that is randomly selected to go at the start of the prompt<br>\n",
        "#@markdown    - I recommend 2 ways to use randomStart:<br>\n",
        "#@markdown  - 1) use a list of multiple emotions or elements *(ex. icy, firey, rainy)*<br>\n",
        "#@markdown  - 2) leave only a single option if you have a consistent theme *(ex. 'female' or 'male')*<br>\n",
        "#<br>\n",
        "#@markdown - 3c):**`midDetails`:** includes keywords to create higher detailed results *(ex. highly detailed, rendered in octane)*<br>\n",
        "#<br>\n",
        "#@markdown - 3d): **`endingStyle`:** includes art styles or artist names *(ex. art by  alphonse mucha)*<br>\n",
        "#<br>\n",
        "#@markdown `NOTE: you don't have to split your prompts with these seperate fields. You can simply write the whole prompt in midDetails or in the batch prompt list too`<br>\n",
        "#<br>\n",
        "#<br>\n",
        "#@markdown  Example of How Prompts Combine: \n",
        "#@markdown - [randomStart] + **[batchPrompt]** + [midDetails] + [endingStyle] + [randomEnd]<br>\n",
        "#@markdown  - `Example 1:` [pearlescent] **[jellyfish in space],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 2:` [pearlescent] **[Lamborghini],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 3:` [glowing] **[frogs on lily pads],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown  - `Example 4:` [sparkling] **[shells on a beach],** [digital art, concept art...], [by Alex Ross...]<br>\n",
        "#@markdown - Notice in the examples the first word is random and the midDetail + endingStyle words are consistent\n",
        "#@markdown - The batch prompts are set up in a list a seperate cell under the variable **promptsA**<br>\n",
        "#<br>\n",
        "\n",
        "3a) useThese3PromptFields = True#@param{type:\"boolean\"}\n",
        "3b) randomStart = \"female\" #@param {type:\"string\"}\n",
        "3c) midDetails = \"digital art, concept art, smooth, sharp focus, high definition, rendered in Octane\" #@param {type:\"string\"}\n",
        "3d) endingStyle = \"illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##4---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 4): Basic Settings** *(required - all modes):* </u><br>\n",
        "<br>\n",
        "#@markdown 4a) **`width and height`:**\n",
        "#@markdown - image dimensions. range of 512-768 is best for portrait. Max is 704 x 704 depending on the GPU <br>\n",
        "#<br>\n",
        "#@markdown 4b) **`aspectRatio`:**\n",
        "#@markdown - Presets for standard image ratios to quickly choose between. Print as well as Film Ratio standards<br>\n",
        "#<br>\n",
        "#@markdown 4c) **`guidance_scale`:**\n",
        "#@markdown - strength of text prompt. 15 is good, 20 can have greater realism. 7 was beta default. Experiment<br>\n",
        "\n",
        "#<br>\n",
        "#@markdown 4d) **`steps`:**\n",
        "#@markdown - number of diffusion steps. more takes longer, but has better detail. test with 5-20 and use 50-80. rarely exceed 100 (costly and insignificant)<br>\n",
        "#<br>\n",
        "#@markdown 4e) **`number_of_repeats`:**\n",
        "#@markdown - how many rounds of iterating through the prompt list. the prompts always run from first to last of each before repeating<br>\n",
        "#<br>\n",
        "#@markdown 4f) **`sampler`:**\n",
        "#@markdown - it seems klms is default, but sometime of my results worked better with ddim, especially portraits<br>\n",
        "#<br>\n",
        "#@markdown 4g) **`seed`:**\n",
        "#@markdown - set -1 to generate by random, otherwise input # seed to regenerate. Make use of randomStart/End with seed for fun variations, or use it for the sequence (same seed = consistent)<br>\n",
        "#<br>\n",
        "#@markdown 4h) **`first_prompt_only`:**\n",
        "#@markdown - will only run on the first prompt in the prompts list<br>\n",
        "#@markdown - good if you want to test on a single image without requiring the deletion of the list<br>\n",
        "#<br>\n",
        "4a) width = 640 #@param {type:\"integer\"}\n",
        "4a) height = 640 #@param {type:\"integer\"}\n",
        "4b) aspectRatio = 'custom' #@param [\"custom\",\"1:1  (square)\",\"4:3  (standard)\",\"16:9 (widescreen)\",\"21:9 (ultrawide)\",\"4:5  (instagram)\",\"8x11 (paper standard)\",\"2:3  (poster standard)\"]\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "4c) guidance_scale = 15 #@param {type:\"slider\", min:0, max:40, step:0.5}\n",
        "4d) steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 1 #param {type:\"integer\"}\n",
        "4e) number_of_repeats = 3 #@param {type:\"integer\"}\n",
        "4f) sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "4g) seed = -1 # @param {type:\"integer\"}\n",
        "4h) firstPromptOnly = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##5---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 5): Init Image (img2img)** *(optional):*</u><br> \n",
        "<br>\n",
        "#@markdown `Note: you must upload the image(s) to the init_image folder on the left Files menu under the / content/init_image/ folder, or change the defaultInitPath`<br>\n",
        "#<br>\n",
        "#@markdown 5a) **`randomizeImageFromFolder:`**\n",
        "#@markdown  - leave unchecked (False) to go through the list of images that are lined up with the prompt\n",
        "#@markdown  - set to True to pull a random image from the **defaultInitPath** to go with your prompts`<br>\n",
        "#<br>\n",
        "#@markdown - some notes on randomization:<br>\n",
        "#@markdown  - if you leave randomization off, make sure your count of images matches the number of prompts in both lists 10 prompts = 10 images<br>\n",
        "#@markdown  - if you randomize the init_images, then you don't have to have the matching # of prompts to image. You could have 3 images in the folder and 100 prompts<br>\n",
        "#<br>\n",
        "#@markdown 5b) **`defaultInitPath:`**\n",
        "#@markdown - where the init_images need to be saved to load. /content/ is fine most of the time\n",
        "#@markdown - a custom drive path is great if you want to save collections on your drive to use randomly across different promps\n",
        "<br>\n",
        "#@markdown 5c) **`initial_image`:**\n",
        "#@markdown - if you want to work with a single image without setting up the list array, then you can enter the image name/extension here<br>\n",
        "#@markdown - otherwise you can leave this field blank<br>\n",
        "#<br>\n",
        "#@markdown 5d) **`init_strength`:** *IMPORTANT* I use this option the most *(since I use img2img the most)*<br>\n",
        "#@markdown - Adjust the intensity of how much the input image influences the result<br>\n",
        "#@markdown - 0 means no input image is used. range 0.1 - 0.3 follows loose for creativity. 0.45 - 0.7 follows image closer for consistency\n",
        "#@markdown - The strength goes in increments of 0.1, however you will find that 0.05 is also a valid and different result in between 0 and 0.1!\n",
        "<br>\n",
        "5a) randomizeImageFromFolder = False#@param{type:\"boolean\"}\n",
        "5b) defaultInitPath = \"/content/init_image/\" #@param {type:\"string\"}\n",
        "5c) initial_image = \"\" #@param {type:\"string\"}\n",
        "init_image_or_folder = \"/content/init_image/\" + initial_image\n",
        "5d) init_strength = 0 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##6---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 6): Sequence Settings - Cycles** *(Through Ranges - optional):* <br></u>\n",
        "<br>\n",
        "#@markdown -  `Note 1: check both init_strength_0_to_1 and cfg_scale_7_to_20 to True to generate a grid of possibilities. 80 generations`<br>\n",
        "#@markdown -  `Note 2: Can iterate on a seed through the scales to see how the values affect results, or randomize:`\n",
        "\n",
        "#@markdown 6a) **`cycleThroughPromptList`:**\n",
        "#@markdown - activate to cycle through the prompt list templates to generate **variations** on the same prompts\n",
        "#<br>\n",
        "#@markdown 6b) **`templatesToCycle`:**\n",
        "#@markdown - when cycleThroughPromptList=True, these are the keys of the Prompt Templates to cycle through\n",
        "#@markdown - example: 1,2 means a female and a male in a digital painting style is created for each prompt (granted you have default prompt templates)\n",
        "#@markdown - the key numbers can be found labeled in the promptTemplateDict variable\n",
        "#<br>\n",
        "#@markdown 6c) **`init_strength_0_to_1`:**\n",
        "#@markdown - Goes through image strength setting 0 - 1 to see its effects. runs only on 1st prompt\n",
        "#<br>\n",
        "#@markdown 6d) **`cfg_scale_7_to_20`:**\n",
        "#@markdown -  Goes through guidance_scale setting 7 - 20 to see its effects. runs only on 1st prompt\n",
        "#<br>\n",
        "#@markdown 6e) **`init_strength_random`:**\n",
        "#@markdown - randomly picks an image strength value from 0.1 to 0.7<br>\n",
        "#<br>\n",
        "#@markdown 6f) **`cfg_scale_random`:**\n",
        "#@markdown - randomly picks an guidance_scale value from 7 to 20<br>\n",
        "#<br>\n",
        "6a) cycleThroughPromptList = False#@param{type:\"boolean\"}\n",
        "6b) templatesToCycle = \"1,2\" #@param {type:\"string\"}\n",
        "# these options would run 14 times:\n",
        "6c) init_strength_0_to_1 = False#@param{type:\"boolean\"}\n",
        "6d) cfg_scale_7_to_20 = False#@param{type:\"boolean\"}\n",
        "6e) init_strength_random = False#@param{type:\"boolean\"}\n",
        "6f) cfg_scale_random = False#@param{type:\"boolean\"}\n",
        "save_settings_file = False#param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##7---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 7): Sequence Settings with Images** *(optional interpolation):* <br></u>\n",
        "<br>\n",
        "#@markdown 7a) **`generatedImageCycle`:**\n",
        "#@markdown - This will cycle through the initial prompt (if firstPromptOnly is True) or the list of prompts with slight variance, creating morphing images!\n",
        "#@markdown - This can create trippy results OR be used to **create subtle variations for concept art / design!**\n",
        "\n",
        "#<br>\n",
        "#@markdown 7b) **`specifyStrengthLock`:**\n",
        "#@markdown - -1 is default to go from 0.6 to 0.2; instead you can lock to only gen say all 0.05 (note: It doesn't blend as well locking to a single value. -1 is best)\n",
        "#<br>\n",
        "#@markdown 7c) **`usePromptsOverTimeCycle`:**\n",
        "#@markdown - Set this to true to use the variable **promptInterpolateMulti** from cell 2\n",
        "#@markdown - In the dictionary of promptInterpolateMulti, you'll be able to set a duration of frames for each prompt and cycle through multiple prompts!\n",
        "#@markdown - This option is fantastic for animated morphs over time. It can create trippy results and unpredictable transformations\n",
        "#<br>\n",
        "#@markdown 7d) **`promptOverTimeSetting03InsteadOf02`:**\n",
        "#@markdown - Set this to True to generate interpolations down to 0.3 instead of 0.2. \n",
        "#@markdown - This will create a more subtle interpolation which will blender better, but will not create as much interest\n",
        "#<br>\n",
        "# for interpolating using generated image cycles on INIT_STRENGTH:\n",
        "7a) generatedImageCycle = False#@param{type:\"boolean\"} \n",
        "7b) specifyStrengthLock = 0.2 #@param {type:\"number\"} # -1 is default to go from 0.6 to 0.2; or you can lock to only gen say 0.05. It doesn't blend as well locking to a single value\n",
        "7c) usePromptsOverTimeCycle = False#@param{type:\"boolean\"}\n",
        "7d) promptOverTimeSetting03InsteadOf02 = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##8---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 8): Research Outputs** *(optional):* <br></u>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGlb8rzxcB7F"
      },
      "source": [
        "### <u>Prompt Storage</u> *(reference only*\n",
        "- this section is for saving useful prompts and subjects that aren't connected to code in any way, they are just here for convenience "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM2cByQdjN_m"
      },
      "outputs": [],
      "source": [
        "# Put each of your prompts on a new line\n",
        "# Examples:\n",
        "'''\n",
        "Animals:\n",
        "a frog relaxing on a lilypad while ribbiting\n",
        "a shiba inu riding a ufo to the moon\n",
        "\n",
        "Characters:\n",
        "wizard with a large brim magic hat and a long robe casting a spell\n",
        "barbarian hunter with a club and an athletic physique\n",
        "'''\n",
        "\n",
        "# Prompt Template Brainstorming:\n",
        "'''\n",
        "extremely detailed oil painting, by rhads, sargent and leyendecker, savrasov levitan polenov, bruce pennington, studio ghibli, tim hildebrandt, digital art, landscape painting, trending on artstation, masterpiece\n",
        "a wholesome motorcycle ride across beautiful scenery, studio Ghibli, Pixar and Disney animation, sharp, Rendered in Redshift and Unreal Engine 5 by Greg Rutkowski, Bloom, dramatic lighting\n",
        "'''\n",
        "\n",
        "# my custom prompt ideas:\n",
        "'''\n",
        "Cheesy Characters:\n",
        "gothic, goth fantasy blacksmith apprentice with magical powers, character design, black hair, pentagram\n",
        "gothic, goth fantasy blacksmith with magical powers, pentagram\n",
        "barbarian hunter with a bow and arrow and an athletic physique\n",
        "wizard with a large brim magic hat and a long robe casting a spell\n",
        "attractive queen of the moon Athena and a starry night\n",
        "female gothic crusader with her face visible, long hair, metal armor, character design, full body\n",
        "\n",
        "Full Prompts:\n",
        "female gothic crusader with her face visible, long hair, metal armor, character design, full body, intricate fine tipped pen drawing, inktober, Fine Line Tattoo, manga line art, monochrome, dotwork, by dan hilliard, by Stanislaw Wilczynski, by alphonse mucha, by aaron horkey\n",
        "\n",
        "Interesting Artists:\n",
        "Alex Grey (psychedelic, trippy)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hwXeJhIfJl"
      },
      "source": [
        "# 2. <u> Batch Prompting</u> *(where the magic takes place, **EXPAND THIS IT!**)*\n",
        "> **Use Cases:** | txt2img (single or batch) | img2img (single or batch) | cycle through cfg/init | sequence animator | prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iPfN1gYgoJkW"
      },
      "outputs": [],
      "source": [
        "#@title # Cell 3: Main Code (the magic button; **RUN IT!**)\n",
        "##@markdown\n",
        "\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "# create a list of all the prompts from new line:\n",
        "#if type(promptsA) is str:\n",
        "#  promptsA = [i for i in promptsA.split('\\n') if i]\n",
        "\n",
        "# UPDATE: 07/09/2022:\n",
        "# I added the prompts to a text file to make it more user friendly to deal with them, so this is now how they're loaded:\n",
        "with open('/content/promptTexts/_promptsMain.txt') as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "promptsA = lines\n",
        "\n",
        "file = open('/content/promptTexts/promptTemplateDictionary.txt', 'r')\n",
        "promptTemplateDict ={}\n",
        "count = 0\n",
        "for line in file:\n",
        "    y = line.split(\"|\")\n",
        "    z = y[1]\n",
        "    b=y[0]\n",
        "    c=y[1]\n",
        "    e=len(c)-1\n",
        "    c=c[0:e]\n",
        "    promptTemplateDict[count]=[b,c]\n",
        "    count+=1\n",
        "#print(promptTemplateDict[0])\n",
        "\n",
        "file = open('/content/promptTexts/promptInterpolateMulti.txt', 'r')\n",
        "promptInterpolateMulti ={}\n",
        "count = 0\n",
        "for line in file:\n",
        "    y = line.split(\"|\")\n",
        "    z = y[1]\n",
        "    b=y[0]\n",
        "    c=y[1]\n",
        "    e=len(c)-1\n",
        "    c=c[0:e]\n",
        "    promptInterpolateMulti[count]=[b,c]\n",
        "    count+=1\n",
        "# print(promptInterpolateMulti[0])\n",
        "\n",
        "file = open('/content/promptTexts/promptEmotiveTemplate.txt', 'r')\n",
        "promptEmotiveTemplateDict ={}\n",
        "count = 0\n",
        "for line in file:\n",
        "    y = line.split(\"|\")\n",
        "    z = y[1]\n",
        "    b=y[0]\n",
        "    c=y[1]\n",
        "    e=len(c)-1\n",
        "    c=c[0:e]\n",
        "    promptEmotiveTemplateDict[count]=[b,c]\n",
        "    count+=1\n",
        "# print(promptEmotiveTemplateDict[0])\n",
        "\n",
        "# ---------------- END READING PROMPT TEMPLATES ----------------------------\n",
        "\n",
        "def clean_string(s):\n",
        "    s = ''.join([c for c in s if (re.match('[a-zA-Z0-9 _]', c) or ord(c) > 127)]).strip()\n",
        "    if len(s) > 200:\n",
        "        return (s[:150]).strip()\n",
        "    return s\n",
        "\n",
        "##1---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 1): Saving/Folder Settings** *(required):* <br></u>\n",
        "outputFolder = \"newidea5\" #@param {type:\"string\"}\n",
        "saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "numberBeginningOfFile = False#@param{type:\"boolean\"}\n",
        "opt.saveNumberPosition = numberBeginningOfFile\n",
        "# note: the actual saving to output directory was moved lower since we might want to save the prompt name there\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##2---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 2): Prompt Template** *(optional):*</u>\n",
        "# <br>\n",
        "#@markdown `Note: set to none if you rather type your prompt details in section 3) |  the selected template is used on EVERY prompt in the promptsA list`\n",
        "promptTemplate = 'none/custom' #@param [\"none/custom\",\"character - female adult\", \"character - male adult\", \"environment - scifi\", \"environment - fantasy\", \"vehicle - sketch\", \"vehicle - render\", \"design - weapon\", \"style - anime keyframe (use high guidance scale of 15+)\", \"style - comic art inks\", \"style - blackwork / heavily inked tattoos\", \"fun - action figure character\", \"fun - stickers\", \"fun - marble statue\"]\n",
        "templateSelected = \"none\"\n",
        "templateSelectedStart = \"\"\n",
        "templateSelectedKey = 0\n",
        "\n",
        "if promptTemplate == \"none/custom\":\n",
        "  templateSelectedKey = 0\n",
        "  templateSelected = \"none\"\n",
        "  templateSelectedStart = \"\"\n",
        "elif promptTemplate == \"character - female adult\":\n",
        "  templateSelectedKey = 1\n",
        "elif promptTemplate == \"character - male adult\":\n",
        "  templateSelectedKey = 2\n",
        "elif promptTemplate == \"environment - scifi\": \n",
        "  templateSelectedKey = 3\n",
        "elif promptTemplate == \"environment - fantasy\": \n",
        "  templateSelectedKey = 4\n",
        "elif promptTemplate == \"vehicle - sketch\": \n",
        "  templateSelectedKey = 5\n",
        "elif promptTemplate == \"vehicle - render\": \n",
        "  templateSelectedKey = 6\n",
        "elif promptTemplate == \"design - weapon\": \n",
        "  templateSelectedKey = 7\n",
        "elif promptTemplate == \"style - anime keyframe (use high guidance scale of 15+)\": \n",
        "  templateSelectedKey = 8\n",
        "elif promptTemplate == \"style - comic art inks\": \n",
        "  templateSelectedKey = 9\n",
        "elif promptTemplate == \"style - blackwork / heavily inked tattoos\": \n",
        "  templateSelectedKey = 10\n",
        "elif promptTemplate == \"fun - action figure character\": \n",
        "  templateSelectedKey = 11\n",
        "elif promptTemplate == \"fun - stickers\": \n",
        "  templateSelectedKey = 12\n",
        "elif promptTemplate == \"fun - marble statue\": \n",
        "  templateSelectedKey = 13\n",
        "\n",
        "# code for determining which key is selected:\n",
        "# templateSelectedKey = 6\n",
        "if templateSelectedKey != 0:\n",
        "  promptTemplatedSelectedlist = promptTemplateDict[int(templateSelectedKey-1)] # gets both the values of the selected key as a list, ex ['male', 'line art']\n",
        "  templateSelectedStart = promptTemplatedSelectedlist[0]\n",
        "  templateSelected = promptTemplatedSelectedlist[1]\n",
        "# end template prompt list ------------------------------------------------------------\n",
        "# ---------------------------\n",
        "\n",
        "emotiveTemplate = 'none' #@param [\"none\", \"happy\", \"angry\", \"sad\", \"zombie\", \"psychedelic\", \"sketchy drawing style\"]\n",
        "emoteTemplateSelected = \"none\"\n",
        "emotiveTemplateSelectedKey = 0\n",
        "\n",
        "if emotiveTemplate == \"none\":\n",
        "  emotiveTemplateSelectedKey = 0\n",
        "  emoteTemplateSelected = \"none\"\n",
        "elif emotiveTemplate == \"happy\":\n",
        "  emotiveTemplateSelectedKey = 1\n",
        "elif emotiveTemplate == \"angry\":\n",
        "  emotiveTemplateSelectedKey = 2\n",
        "elif emotiveTemplate == \"sad\":\n",
        "  emotiveTemplateSelectedKey = 3\n",
        "elif emotiveTemplate == \"zombie\":\n",
        "  emotiveTemplateSelectedKey = 4\n",
        "elif emotiveTemplate == \"psychedelic\":\n",
        "  emotiveTemplateSelectedKey = 5\n",
        "elif emotiveTemplate == \"sketchy drawing style\":\n",
        "  emotiveTemplateSelectedKey = 6\n",
        "\n",
        "# code for determining which key is selected:\n",
        "# templateSelectedKey = 6\n",
        "if emotiveTemplateSelectedKey != 0:\n",
        "  promptTemplatedSelectedlist = promptEmotiveTemplateDict[int(emotiveTemplateSelectedKey-1)] # gets both the values of the selected key as a list, ex ['male', 'line art']\n",
        "  emoteTemplateSelected = promptTemplatedSelectedlist[0]\n",
        "# end template prompt list ------------------------------------------------------------\n",
        "# ---------------------------\n",
        "\n",
        "\n",
        "# end template emotive addition list -----------------------------------------------------------\n",
        "\n",
        "randomEnd = \"\" #@param {type:\"string\"}\n",
        "randomStringEnd = randomEnd.split(\",\")\n",
        "numOfRandomStringEnd = len(randomStringEnd)\n",
        "wordRNDMend = \"\"\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##3---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 3): Prompt Details** *(optional - toggle on/off):* <br></u>\n",
        "# <br>\n",
        "#@markdown `Note: these values are included on EVERY prompt in the list if no template is used and useThese3PromptFields = True`\n",
        "useThese3PromptFields = False#@param{type:\"boolean\"}\n",
        "randomStart = \"intricate pen drawing by Aaron Horkey\" #@param {type:\"string\"}\n",
        "if useThese3PromptFields == False and promptTemplate != \"none/custom\":\n",
        "  randomStart = \"\"\n",
        "wordRNDM = \"\"\n",
        "randomStringLS = randomStart.split(\",\")\n",
        "numOfRandomStringStart = len(randomStringLS)\n",
        "midDetails = \"Inktober, halloween, Fine Line Tattoo, line art, monochrome, by Stanislaw Wilczynski\" #@param {type:\"string\"}\n",
        "endingStyle = \"\" #@param {type:\"string\"}\n",
        "if useThese3PromptFields == False and promptTemplate != \"none/custom\":\n",
        "  midDetails = \"\"\n",
        "  endingStyle = \"\"\n",
        "# next we determine if we should actually use these custom strings or use our prompt templates:\n",
        "promptsB = \"\"\n",
        "if templateSelectedKey == 0:\n",
        "  promptsB = [x + \", \" + midDetails for x in promptsA] # add the universal style text to each prompt in the list\n",
        "  prompts = [x + \", \" + endingStyle for x in promptsB] # add the universal style text to each prompt in the list\n",
        "else:\n",
        "  prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "if promptTemplate == \"none/custom\":\n",
        "  useThese3PromptFields = True\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##4---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 4): Basic Settings** *(required - all modes):* </u><br>\n",
        "#<br>\n",
        "#@markdown `Note: Defaults: width/height = 640/640 | scale = 15 | steps = 50 | repeats = 6 | sampler = ddim | seed = -1 (random)`\n",
        "width = 672 #@param {type:\"integer\"}\n",
        "height = 672 #@param {type:\"integer\"}\n",
        "aspectRatio = '4:5  (instagram)' #@param [\"custom\",\"1:1  (square)\",\"4:3  (standard)\",\"16:9 (widescreen)\",\"21:9 (ultrawide)\",\"4:5  (instagram)\",\"8x11 (paper standard)\",\"2:3  (poster standard)\"]\n",
        "if aspectRatio == \"custom\":\n",
        "  width = width\n",
        "  height = height\n",
        "elif  aspectRatio == \"1:1  (square)\":\n",
        "  width = 640\n",
        "  height = 640\n",
        "elif  aspectRatio == \"4:3  (standard)\":\n",
        "  width = 768\n",
        "  height = 576\n",
        "elif  aspectRatio == \"16:9 (widescreen)\":\n",
        "  width = 832\n",
        "  height = 480\n",
        "elif  aspectRatio == \"21:9 (ultrawide)\":\n",
        "  width = 1024\n",
        "  height = 448\n",
        "elif  aspectRatio == \"4:5  (instagram)\":\n",
        "  width = 512\n",
        "  height = 640\n",
        "elif  aspectRatio == \"8x11 (paper standard)\":\n",
        "  width = 512\n",
        "  height = 672\n",
        "elif  aspectRatio == \"2:3  (poster standard)\":\n",
        "  width = 512\n",
        "  height = 768\n",
        "\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "guidance_scale = 16 #@param {type:\"slider\", min:0, max:40, step:1}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 1 #param {type:\"integer\"}\n",
        "number_of_repeats = 30 #@param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "firstPromptOnly = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##5---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 5): Init Image (img2img)** *(optional):*</u><br> \n",
        "#<br>\n",
        "#@markdown `Note: the default path is: / content/init_image/`\n",
        "randomizeImageFromFolder = False#@param{type:\"boolean\"}\n",
        "defaultInitPath = \"/content/init_image/\" #@param {type:\"string\"}\n",
        "if defaultInitPath == \"\":\n",
        "  defaultInitPath = \"/content/init_image/\"\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(defaultInitPath):\n",
        "    if os.path.isfile(os.path.join(defaultInitPath, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "# print(numberOfImages)\n",
        "# print(init_imgs_list)\n",
        "\n",
        "# create a list of all the init images from new line:\n",
        "if type(init_imgs_list) is str:\n",
        "  init_imgs_list = [i for i in init_imgs_list.split('\\n') if i]\n",
        "if len(init_imgs_list) == 0:\n",
        "  init_imgs_list.append(\"none\")\n",
        "  \n",
        "initial_image = \"\" #@param {type:\"string\"}\n",
        "checkForInitial_Image = len(initial_image)\n",
        "if checkForInitial_Image == 0:\n",
        "  init_image_or_folder = defaultInitPath\n",
        "else:\n",
        "  init_image_or_folder = defaultInitPath + initial_image\n",
        "init_strength = 0 #@param {type:\"number\"}\n",
        "cycleInitStrengthChangeAmount = 0.05 #@param {type:\"slider\", min:0.01, max:0.2, step:0.01}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "##6---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 6): Sequence Settings - Cycles** *(Through Ranges - optional):* <br></u>\n",
        "#<br>\n",
        "#@markdown `Note: use cycleThroughPromptList to create variations with the prompt templates`\n",
        "# for template cycling, ex generating both male and female or sketch after the other:\n",
        "cycleThroughPromptList = False#@param{type:\"boolean\"}\n",
        "templatesToCycle = \"1,2\" #@param {type:\"string\"}\n",
        "listOfTemplatesToCycle = templatesToCycle.split(\",\")\n",
        "numOfTemplatesToCycle = len(listOfTemplatesToCycle)\n",
        "# end template cycle variables-----\n",
        "# these options would run 14 times:\n",
        "init_strength_0_to_1 = False#@param{type:\"boolean\"}\n",
        "cfg_scale_7_to_20 = False#@param{type:\"boolean\"}\n",
        "init_strength_random = False#@param{type:\"boolean\"}\n",
        "cfg_scale_random = False#@param{type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "##7---------------------------------------------------------------------------------------------\n",
        "#@markdown > <u>**Section 7): Sequence Settings with Images** *(optional interpolation):* <br></u>\n",
        "#<br>\n",
        "# forgot what the next one does, it wasn't as good as doing the image cycle though:\n",
        "useGeneratedImageAsInit = False#param{type:\"boolean\"}\n",
        "# for interpolating using generated image cycles on INIT_STRENGTH:\n",
        "generatedImageCycle = False#@param{type:\"boolean\"} \n",
        "specifyStrengthLock = -1 #@param {type:\"number\"} # -1 is default to go from 0.6 to 0.2; or you can lock to only gen say 0.05. It doesn't blend as well locking to a single value\n",
        "activateStrengthLock = False\n",
        "if specifyStrengthLock == -1:\n",
        "  activateStrengthLock = False\n",
        "else:\n",
        "  activateStrengthLock = True\n",
        "# prompts over time lets us pick keyframe for duration of certain prompts\n",
        "usePromptsOverTimeCycle = False#@param{type:\"boolean\"}\n",
        "# sometimes there's too large a jump when we go down to 0.2 inti strngth so this has smoother results:\n",
        "promptOverTimeSetting03InsteadOf02 = False#@param{type:\"boolean\"}\n",
        "promptsOverTimeFramesList = []\n",
        "promptsOverTimePromptList = []\n",
        "totalFrames = 0\n",
        "# if we're using prompts over time, we take the user created DICT of frame count / prompts and split to new list\n",
        "# based on frame count, we'll duplicate that prompt into the prompt list the rest of the script will read\n",
        "if usePromptsOverTimeCycle == True:\n",
        "  firstPromptOnly = False\n",
        "  prompts.clear()\n",
        "  promptsA.clear()\n",
        "  promptsB = \"\"\n",
        "  for key, values in promptInterpolateMulti.items():\n",
        "     number = int(values[0]) # frame count\n",
        "     promptsOverTimeFramesList.append(number) # add to list of frame counts, ex. 6\n",
        "     promptsOverTimePromptList.append(values[1]) # adds to list of prompt strings, ex. 'cat'\n",
        "  for x in promptsOverTimeFramesList:\n",
        "    totalFrames += int(x) # this value will be the total of the prompts / frames we gen\n",
        "  number_of_repeats = totalFrames # we might as well pick an accurate generation count to match our frame count. so override this user inputted variable\n",
        "  counta = 0\n",
        "  zz = 0\n",
        "  for y in range(totalFrames):\n",
        "    promptToSend = promptsOverTimePromptList[zz]\n",
        "    promptsA.append(promptToSend)\n",
        "    counta += 1\n",
        "    if counta == (promptsOverTimeFramesList[zz]):\n",
        "      zz += 1\n",
        "      counta = 0\n",
        "  if templateSelectedKey == 0:\n",
        "    promptsB = [x + \", \" + midDetails for x in promptsA] # add the universal style text to each prompt in the list\n",
        "    prompts = [x + \", \" + endingStyle for x in promptsB] # add the universal style text to each prompt in the list\n",
        "  else:\n",
        "    prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "# ----- end set up of the usePromptsOverTimeCycle / multi prop with frame count cycle interpolation setup -----------------\n",
        "\n",
        "save_settings_file = True#param{type:\"boolean\"} # it seems to break script if this is set to False, so I hid it\n",
        "\n",
        "# Sequence Counter for evaluating cfg and init_strength scales:\n",
        "areBothScalesToGen = False\n",
        "if init_strength_0_to_1 == True:\n",
        "  firstPromptOnly = True\n",
        "  if cfg_scale_7_to_20 == True:\n",
        "    number_of_repeats = 80 # value if both options are selected\n",
        "    areBothScalesToGen = True\n",
        "  else:\n",
        "    number_of_repeats = 10\n",
        "if cfg_scale_7_to_20 == True:\n",
        "  firstPromptOnly = True\n",
        "  if init_strength_0_to_1 == True:\n",
        "    number_of_repeats = 80 # value if both options are selected\n",
        "    areBothScalesToGen = True\n",
        "  else:\n",
        "    number_of_repeats = 8\n",
        "counterCFG = 6\n",
        "counterINITSTRENGTH = 0\n",
        "counter = 0\n",
        "countRepeats = -1\n",
        "number_of_repeats_display = number_of_repeats\n",
        "randomNumber = 0\n",
        "countPromptTemplateCycles = -1\n",
        "countSubfolderNumber = 0\n",
        "sample_idx = 1 # reset this number every generation\n",
        "\n",
        "# INIT_Image gen from drive:\n",
        "activateUsePreviousGenImage = False # we have to activate this later in the code\n",
        "areWeOnOddInitGenCycle = True # odd will gen random seed at 0.3, even will fen previous seed at 0.7\n",
        "seedA_GenCycle = random.randint(0, 2**32) if seed == -1 else seed\n",
        "seedB_GenCycle = random.randint(0, 2**32)\n",
        "reset_init_strength_for_gen_cycle = False\n",
        "if generatedImageCycle == True:\n",
        "  init_strength = 0.9 #starting value for cycle; we go from 0.6 down to 0.2 in 0.1 increments\n",
        "  init_strength_0_to_1 = False\n",
        "  cfg_scale_7_to_20 = False\n",
        "\n",
        "if firstPromptOnly == True:\n",
        "  number_of_repeats_display = number_of_repeats\n",
        "else:\n",
        "  number_of_repeats_display = number_of_repeats * len(prompts)\n",
        "\n",
        "# check if we're cycling through prompt templates:\n",
        "if cycleThroughPromptList == True:\n",
        "  if init_strength_0_to_1 == False and cfg_scale_7_to_20 == False:\n",
        "    if usePromptsOverTimeCycle == False:\n",
        "      number_of_repeats = numOfTemplatesToCycle * len(prompts)\n",
        "      number_of_repeats_display = number_of_repeats\n",
        "    elif usePromptsOverTimeCycle == True:\n",
        "      number_of_repeats = numOfTemplatesToCycle\n",
        "      number_of_repeats_display = number_of_repeats\n",
        "\n",
        "if usePromptsOverTimeCycle == True:\n",
        "  number_of_repeats_display = number_of_repeats\n",
        "  number_of_repeats = 1 # we only repeat once since we already compiled all our prompts\n",
        "\n",
        "# --------- CODE STARTS GENERATING ----------------\n",
        "for x in range(number_of_repeats):\n",
        "  countRepeats +=1\n",
        "  countSubfolderNumber = 0\n",
        "  # These counters count every time the prompt is sent for another repeat\n",
        "  if firstPromptOnly == True:\n",
        "    if init_strength_0_to_1 == False or cfg_scale_7_to_20 == False:\n",
        "      counter = counter\n",
        "  else:\n",
        "    if init_strength_0_to_1 == False:\n",
        "      counter = 0\n",
        "    if cfg_scale_7_to_20 == False:\n",
        "      counter = 0\n",
        "  if init_strength_0_to_1 == True or cfg_scale_7_to_20 == True:\n",
        "    if countRepeats == number_of_repeats:\n",
        "      break\n",
        "\n",
        "  # check if we're cycling through prompt templates and need to swap the prompt text:\n",
        "  if cycleThroughPromptList == True:\n",
        "    countPromptTemplateCycles += 1\n",
        "    if templateSelectedKey == number_of_repeats: # prevents the list going out of range\n",
        "      break\n",
        "    try:\n",
        "      templateSelectedKey = listOfTemplatesToCycle[countPromptTemplateCycles] # for some reason this line turns the into to string, so have to turn it back to int\n",
        "    except:\n",
        "      break\n",
        "    promptTemplatedSelectedlist = promptTemplateDict[int(templateSelectedKey)] # gets both the values of the selected key as a list, ex ['male', 'line art']\n",
        "    templateSelectedStart = promptTemplatedSelectedlist[0]\n",
        "    templateSelected = promptTemplatedSelectedlist[1]\n",
        "    prompts = [x + \", \" + templateSelected for x in promptsA] # add the universal style text to each prompt in the list\n",
        "\n",
        "\n",
        "  # calculating the full prompt: ----------------------------------\n",
        "  for prompt in prompts:\n",
        "    # check if we are only supposed to generate the first prompt or all them:\n",
        "    if firstPromptOnly == True:\n",
        "      try:\n",
        "        if prompt == prompts[1]:\n",
        "          break\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    # the random start and end words are broken, for some reason\n",
        "    if numOfRandomStringStart != 0 and numOfRandomStringStart != 1:\n",
        "      # wordRNDM = random.choice(randomStringLS) # trash line didn't run properly\n",
        "      number = ((seed - seed) + (numOfRandomStringStart - 1))\n",
        "      randomNumber = random.randint(0, number)\n",
        "      # print(numOfRandomStringStart)\n",
        "      if randomNumber > numOfRandomStringStart or randomNumber <0:\n",
        "        randomNumber = 0\n",
        "      wordRNDM = randomStringLS[randomNumber]\n",
        "    elif numOfRandomStringStart == 1:\n",
        "      wordRNDM = randomStringLS[0]\n",
        "    #print(randomNumber)\n",
        "    #print(wordRNDM)\n",
        "\n",
        "    if numOfRandomStringEnd != 0 and numOfRandomStringEnd != 1:\n",
        "      # wordRNDMend = random.choice(randomStringEnd) # this trash didn't work for some reason\n",
        "      number = ((seed - seed) + (numOfRandomStringEnd - 1))\n",
        "      randomNumber = random.randint(0, number)\n",
        "      if randomNumber > numOfRandomStringEnd or randomNumber <0:\n",
        "        randomNumber = 0\n",
        "      wordRNDMend = randomStringEnd[randomNumber]\n",
        "    elif numOfRandomStringEnd == 1:\n",
        "      wordRNDMend = randomStringEnd[0]\n",
        "\n",
        "    if templateSelected != \"none\":\n",
        "      randomStringLS = templateSelectedStart\n",
        "      wordRNDM = templateSelectedStart\n",
        "\n",
        "    # opt.prompt = prompt\n",
        "    if numOfRandomStringStart == 0:\n",
        "      if numOfRandomStringEnd == 0:\n",
        "        promptPreFinal = prompt\n",
        "      else:\n",
        "        if len(wordRNDMend) == 0:\n",
        "          promptPreFinal = prompt\n",
        "        else:\n",
        "          promptPreFinal = prompt + \",\" + wordRNDMend\n",
        "    else:\n",
        "      if numOfRandomStringEnd == 0:\n",
        "        promptPreFinal = wordRNDM + \", \" + prompt\n",
        "      else:\n",
        "        if len(wordRNDMend) == 0:\n",
        "          promptPreFinal = wordRNDM + \", \" + prompt\n",
        "        else:\n",
        "          promptPreFinal = wordRNDM + \", \" + prompt + \",\" + wordRNDMend\n",
        "\n",
        "    # finally bring the combined prompt together:\n",
        "    if emoteTemplateSelected == \"none\":\n",
        "      opt.prompt = promptPreFinal\n",
        "    else:\n",
        "      opt.prompt = promptPreFinal + \" \" + emoteTemplateSelected\n",
        "\n",
        "    shortenedPromptNameList = prompt.split()[:3]\n",
        "    shortenedPromptNameStr = \" \".join(shortenedPromptNameList)\n",
        "    batch_name = clean_string(shortenedPromptNameStr) # shorten the prompt word since the prompts are too long for windows!\n",
        "    if generatedImageCycle == True:\n",
        "      amountOfPaddingForN = len(str(number_of_repeats)) # how much padding to get\n",
        "      xNum = f'{sample_idx:{0}{amountOfPaddingForN}}'\n",
        "      paddedBatchName = str(xNum) + \" \" + batch_name\n",
        "      batch_name = paddedBatchName\n",
        "\n",
        "    # Related to INIT IMAGES:\n",
        "    if generatedImageCycle == False:\n",
        "      full_init_img_path = init_image_or_folder\n",
        "      opt.init_img = full_init_img_path\n",
        "    if init_imgs_list[0] != \"none\":\n",
        "      # opt.outdir = os.path.join(outputs_path, batch_name) # if you want output dir to match the prompt\n",
        "      if randomizeImageFromFolder == True and useGeneratedImageAsInit == False and generatedImageCycle == False:\n",
        "        opt.init_img = init_image_or_folder\n",
        "      if randomizeImageFromFolder == False and generatedImageCycle == False:\n",
        "        if checkForInitial_Image == 0: # we have to check if user wants to run a single specific image (meaning the field in the init image area)\n",
        "          if firstPromptOnly == False:\n",
        "            if init_strength == 0:\n",
        "              print(\"\")\n",
        "              # full_init_img_path = init_image_or_folder + init_imgs_list[counter]\n",
        "            else:\n",
        "              full_init_img_path = init_image_or_folder + init_imgs_list[counter]\n",
        "          else:\n",
        "            full_init_img_path = init_image_or_folder + init_imgs_list[0]\n",
        "        else:\n",
        "          full_init_img_path = init_image_or_folder\n",
        "        print(\"Init image used: \" + full_init_img_path)\n",
        "        opt.init_img = full_init_img_path\n",
        "\n",
        "\n",
        "    opt.ddim_steps = steps\n",
        "    opt.n_iter = 1\n",
        "    opt.n_samples = samples_per_batch\n",
        "\n",
        "    # seed creation and the cycle init image gen from itself:\n",
        "    if generatedImageCycle == False:\n",
        "      # pick random seed for images or the seed specified:\n",
        "      opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "    else:\n",
        "      # this only activates if we are trying to cycle through INIT image and reuse it from the generated location:\n",
        "      # first gen only:\n",
        "      if activateUsePreviousGenImage == False:\n",
        "        opt.init_img = init_image_or_folder # do only on the first generation to get our input image\n",
        "        activateUsePreviousGenImage = True\n",
        "\n",
        "      # core cycle:\n",
        "      init_strength_endPoint = 0.2 # how far we go from 0.6 to 0.2 or 0.3\n",
        "      if promptOverTimeSetting03InsteadOf02 == True:\n",
        "          init_strength_endPoint = 0.3 # for a more subtle interpolation\n",
        "      if areWeOnOddInitGenCycle == True:\n",
        "        opt.seed = seedA_GenCycle\n",
        "        if activateStrengthLock == False:\n",
        "          init_strength -= cycleInitStrengthChangeAmount # only go down 0.1 if there is no strength lock\n",
        "          round(init_strength, 2)\n",
        "          if init_strength == init_strength_endPoint or init_strength <= (init_strength_endPoint + 0.1): # when we get to the end; either 0.2 or 0.3\n",
        "            areWeOnOddInitGenCycle = False\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedA_GenCycle +=1\n",
        "        elif activateStrengthLock == True:\n",
        "          areWeOnOddInitGenCycle = False\n",
        "          init_strength = specifyStrengthLock\n",
        "          reset_init_strength_for_gen_cycle = True\n",
        "          seedA_GenCycle +=1\n",
        "\n",
        "      elif areWeOnOddInitGenCycle == False:\n",
        "        # opt.seed is previous result\n",
        "        opt.seed = seedB_GenCycle\n",
        "        if activateStrengthLock == False:\n",
        "          init_strength -= cycleInitStrengthChangeAmount # the value used to be 0.1 strength down per cycle, but changed it to a variable in case we can actually go more subtle\n",
        "          round(init_strength, 2)\n",
        "          if init_strength == init_strength_endPoint or (init_strength_endPoint + 0.1):\n",
        "            areWeOnOddInitGenCycle = True\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedB_GenCycle +=1\n",
        "        elif activateStrengthLock == True:\n",
        "            areWeOnOddInitGenCycle = True\n",
        "            init_strength = specifyStrengthLock\n",
        "            reset_init_strength_for_gen_cycle = True\n",
        "            seedB_GenCycle +=1     \n",
        "          # ---------- END THE CYCLE THROUGH INIT --------------------------\n",
        "      \n",
        "    # choose to pull one of the generated images as INIT Image:\n",
        "    if useGeneratedImageAsInit == True and generatedImageCycle == False:\n",
        "      if activateUsePreviousGenImage == False:\n",
        "        activateUsePreviousGenImage = True\n",
        "      elif activateUsePreviousGenImage == True:\n",
        "        # this will only run when activateUsePreviousGenImage = True as set above\n",
        "        if generatedImageCycle == False:\n",
        "          while not os.path.exists(lastGeneratedImagePath):\n",
        "            time.sleep(1) \n",
        "          if os.path.isfile(lastGeneratedImagePath):\n",
        "            opt.init_img = lastGeneratedImagePath\n",
        "            seed += 1 # this technique only works if you use different seeds\n",
        "\n",
        "    # CFG Calculation:    \n",
        "    opt.sampler = sampler\n",
        "    if cfg_scale_7_to_20 == False:\n",
        "      opt.scale = guidance_scale\n",
        "    else:\n",
        "      opt.scale = counterCFG\n",
        "    if cfg_scale_random == True:\n",
        "      opt.scale = random.uniform(7, 20)\n",
        "\n",
        "    # INIT IMAGE:\n",
        "    if init_strength_0_to_1 == True:\n",
        "      init_strength = counterINITSTRENGTH\n",
        "    if init_strength_random == True:\n",
        "      init_strength = random.uniform(0.1, 0.8)\n",
        "    opt.strength = round(max(0.0, min(1.0, 1.0 - init_strength)), 2)\n",
        "\n",
        "    #opt.W = width_height[0]\n",
        "    #opt.H = width_height[1]\n",
        "    opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "    # Counters:\n",
        "    counter += 1 # I added this to keep track of INIT images\n",
        "    if firstPromptOnly == True:\n",
        "      if cycleThroughPromptList == True or usePromptsOverTimeCycle == True:\n",
        "        print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle))\n",
        "      else: \n",
        "        print(\"Generation processing: \" + str(counter) + \" / \" + str(number_of_repeats_display))    \n",
        "    else:\n",
        "        displayCounter = (len(prompts) * countRepeats) + counter\n",
        "        if cycleThroughPromptList == True:\n",
        "          print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle * len(prompts)))\n",
        "        else:        \n",
        "          print(\"Generation processing: \" + str(displayCounter) + \" / \" + str(number_of_repeats_display))\n",
        "    if areBothScalesToGen == False:\n",
        "      counterCFG += 2 # go up from 7 to 20 if activated\n",
        "      counterINITSTRENGTH = counterINITSTRENGTH + 0.1 # go up from 0.1 to 0.8 if activated\n",
        "    else:\n",
        "      counterINITSTRENGTH = counterINITSTRENGTH + 0.1\n",
        "      if counterINITSTRENGTH > 0.9:\n",
        "        counterINITSTRENGTH = 0 # reset since we're now going to go to next CFG\n",
        "        counterCFG += 2 # go up from 7 to 20 if activated\n",
        "\n",
        "    if opt.strength >= 1 or init_image_or_folder == None or opt.strength <= 0:\n",
        "        opt.init_img = \"\"\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        opt.sampler = 'ddim'\n",
        "\n",
        "    if opt.sampler != 'ddim':\n",
        "        opt.ddim_eta = 0.0\n",
        "\n",
        "    # debugging during processing:\n",
        "    print(\"prompt: \" + opt.prompt[:110] +\"...\")\n",
        "\n",
        "    # save settings\n",
        "    settings = {\n",
        "        'ddim_eta': ddim_eta,\n",
        "        'guidance_scale': guidance_scale,\n",
        "        'init_image': init_image_or_folder,\n",
        "        'init_strength': init_strength,\n",
        "        'number_of_images': number_of_images,\n",
        "        'prompt': opt.prompt,\n",
        "        'sampler': sampler,\n",
        "        'samples_per_batch': samples_per_batch,\n",
        "        'seed': opt.seed,\n",
        "        'steps': steps,\n",
        "        'width': opt.W,\n",
        "        'height': opt.H,\n",
        "    }\n",
        "    # saving and making output directories:\n",
        "    countSubfolderNumber += 1\n",
        "    if saveSeperatePromptsToSubfolders == False:\n",
        "      opt.outdir = os.path.join(outputs_path, outputFolder)\n",
        "    else:\n",
        "      numToUsePath = countSubfolderNumber.zfill(3)\n",
        "      opt.outdir = os.path.join(outputs_path, outputFolder + \"/\" + str(numToUsePath) + \" \" + shortenedPromptNameStr)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    # filePathToSaveTo = opt.outdir + \"/\" + batch_name + \"\" + batch_idx.zfill(3)  + \"_\" + sample_idx.zfill(4) + \" \" + {opt.seed} + \".png\"\n",
        "    pathBatchIDPad = (f'{batch_idx:03}')\n",
        "    pathsample_idx = (f'{sample_idx:04}') \n",
        "    if numberBeginningOfFile == False:\n",
        "      lastGeneratedImagePath = opt.outdir + \"/\" + batch_name + \"(\" + str(pathBatchIDPad)  + \")_\" + str(pathsample_idx) + \" \" + str(opt.seed) + \".png\"\n",
        "    else:\n",
        "      lastGeneratedImagePath = opt.outdir + \"/\" + str(pathsample_idx) + \" \" + batch_name + \"(\" + str(pathBatchIDPad)  + \")\" + \" \" + str(opt.seed) + \".png\"\n",
        "    # while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_{sample_idx:04}_settings.txt\"):\n",
        "        #countSubfolderNumber += 1\n",
        "        # print( \"counting the prompt: \" + str(countSubfolderNumber))\n",
        "    if save_settings_file:\n",
        "      if numberBeginningOfFile == False:\n",
        "        with open(f\"{opt.outdir}/{batch_name}({batch_idx})_{sample_idx:04}_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "      else:\n",
        "        with open(f\"{opt.outdir}/{sample_idx:04} {batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "    with open(f\"{opt.outdir}/prompt.txt\", 'w') as f:\n",
        "          f.write(opt.prompt)\n",
        "    # sample_idx = 0\n",
        "\n",
        "    for i in range(number_of_images):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        images = generate(opt)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Used seed: {opt.seed}\")\n",
        "        if firstPromptOnly == True:\n",
        "          if cycleThroughPromptList == True or usePromptsOverTimeCycle == True:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle))\n",
        "          else:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(number_of_repeats_display))\n",
        "        else:\n",
        "          if cycleThroughPromptList == True:\n",
        "            print(\"Generations processed: \" + str(counter) + \" / \" + str(numOfTemplatesToCycle * len(prompts)))\n",
        "          else:\n",
        "            print(\"Generations processed: \" + str(displayCounter) + \" / \" + str(number_of_repeats_display))\n",
        "        # print(f\"Saved to: {opt.outdir}\")\n",
        "        print(\"prompt: \" + opt.prompt[:110] +\"...\")\n",
        "        # print(\"Path To Last Image: \" + lastGeneratedImagePath)\n",
        "        # print (\"countdown of generatedImageCycle: \" + str(init_strength))\n",
        "\n",
        "        #for image in images:\n",
        "        #    display(image)\n",
        "        display(images[0])\n",
        "        copy1OfImage = images[0]\n",
        "        copy2OfImage = images[0]\n",
        "        copy1OfImage = copy1OfImage.save(\"/content/lastGeneratedImage/lastGeneratedImage.png\")\n",
        "        lastGeneratedImagePath = \"/content/lastGeneratedImage/lastGeneratedImage.png\"\n",
        "\n",
        "        batch_idx += 1\n",
        "        sample_idx += 1\n",
        "        # opt.seed += 1\n",
        "\n",
        "    # change some settings like swap to previous generated image to cycle though generatedImageCycle:  \n",
        "    if generatedImageCycle == True:\n",
        "      if reset_init_strength_for_gen_cycle == True:\n",
        "        reset_init_strength_for_gen_cycle = False\n",
        "        init_strength = 0.80\n",
        "        copy2OfImage = copy2OfImage.save(\"/content/lastGeneratedImage/lastGeneratedImageForCycle.png\")\n",
        "        lastGeneratedImagePathCycle = \"/content/lastGeneratedImage/lastGeneratedImageForCycle.png\"\n",
        "        \n",
        "        print(\"wait while loading... \" + lastGeneratedImagePathCycle)\n",
        "        while not os.path.exists(lastGeneratedImagePathCycle):\n",
        "          time.sleep(1) \n",
        "        if os.path.isfile(lastGeneratedImagePathCycle):\n",
        "          opt.init_img = lastGeneratedImagePathCycle\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx5EjEc5bgRV"
      },
      "source": [
        "# Bonus Tools *(stand alone by section header)*\n",
        "- | Upscaling (HD+ Tiles) | Facial Recognition | Upscaling (Real ESRGAN) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzsZgcm37USQ"
      },
      "source": [
        "## <u> A) Post Processing - **Grid Upscaling** </u> *(Tile Generation)* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WT-KgMa_qvKd"
      },
      "outputs": [],
      "source": [
        "# CODE BLOCK 1) Split an image into grids of 512x512 size\n",
        "#@title 1) Upscaling and HD Resolution Tile Generator:\n",
        "#@markdown `Note: this script requires a final image as input. It'll split the original image into 512x512 tiles`<br>\n",
        "#@markdown `You can use the optional Grid Preview below to see how the tiles look beforehand`<br>\n",
        "#@markdown `Warning: Don't use an input image with too large of a width. The GPU may crash at 3072+. 2048 pixel width for portraits is good`<br>\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "from itertools import product\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "from google.colab import output\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "googleDriveFolderName = \"test7\" #@param {type:\"string\"}\n",
        "inputImage = \"start.png\" #@param {type:\"string\"}\n",
        "inputDirectory = \"/content/init_image\" #@param {type:\"string\"}\n",
        "outputDirectory = \"/content/HD/preProcessedGridSplits\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/preProcessedGridSplits/**/*', recursive=True)\n",
        "#saveSeperatePromptsToSubfolders = False#@param{type:\"boolean\"}\n",
        "\n",
        "def splitImageTiles(filename, dir_in): # (inputImage, inputDirectory)\n",
        "    # setup:\n",
        "    d = 512 # the width / height we'll split our image into; it's 512 since this is the ideal size to input anyways & the math ONLY works with 512x512 size\n",
        "    # dir_out = dir_in + \"\\\\\" + \"outSplitGrids\"\n",
        "    dir_out = outputDirectory\n",
        "\n",
        "    # clean up existing files in the output Directory:\n",
        "    for f in outputDirectoryFiles:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except OSError as e:\n",
        "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
        "\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    img = Image.open(os.path.join(dir_in, filename))\n",
        "    w, h = img.size # image size, ex (2048, 3072)\n",
        "    #print(w)\n",
        "\n",
        "    # modify the width of the image to work well with the code as a power of 256:\n",
        "    ratioSize = d/2 # the ratio we want to match is half of our image width\n",
        "    nearest_multipleOf256 = int(ratioSize * round(w/ratioSize))\n",
        "    wpercent = (nearest_multipleOf256 / float(img.size[0]))\n",
        "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
        "    img = img.resize((nearest_multipleOf256, hsize), Image.ANTIALIAS) # resize the image so that the width at least matches clean to our grid\n",
        "    # img.save(dir_in + 'resized_image.png')\n",
        "    w, h = img.size # image size, ex (2048, 3072)\n",
        "    outputGridImagesList = []\n",
        "\n",
        "    adjustInitialSize = True #@param{type:\"boolean\"}\n",
        "    inputImageWidthAdjustment = 1024 #@param{type:\"integer\"}\n",
        "    global reducePromptCountby4X\n",
        "    reducePromptCountby4X = True#@param{type:\"boolean\"}\n",
        "\n",
        "    if adjustInitialSize == True:\n",
        "      wpercent = (inputImageWidthAdjustment / float(img.size[0]))\n",
        "      hsize = int((float(img.size[1]) * float(wpercent)))\n",
        "      imgResized = img.resize((inputImageWidthAdjustment, hsize), Image.ANTIALIAS)\n",
        "      w, h = imgResized.size\n",
        "      img = imgResized\n",
        "\n",
        "    ratioW = w / d\n",
        "    ratioH = h / d\n",
        "    ratioHInitial = ratioH\n",
        "\n",
        "    ratioW = int(ratioW) # for example 4\n",
        "    ratioH = int(ratioH) # for example 6\n",
        "    # for some reason int rounds them all down, so we have to accomodate for these ratio possibilities\n",
        "    # 1) images can be less than 0.05 difference, 2) images can be between 0.1 and 0.5 difference and 3) images can be greater than (0.5-0.99) difference\n",
        "    cutsW = ratioW*2 # the amount of cut images on the wdith; the columns ; for example 8\n",
        "    if ratioHInitial - ratioH < 0.05:\n",
        "        cutsH = ratioH*2 # the amount of cut images on the height; the rows ; for example 12\n",
        "    elif ratioHInitial - ratioH >= 0.5:\n",
        "        cutsH = int((ratioH*2) + 2) # the amount of cut images on the height; the rows ; for example 12\n",
        "    else:\n",
        "        cutsH = int((ratioH*2) + 1)\n",
        "    countW = 0\n",
        "    countH = 1 # we start with 1 since we will start in the first row and we're counting by width / countW\n",
        "    image_counter = 0\n",
        "\n",
        "    # before processing the images, check if a folder to store the new images exists:\n",
        "    isExist = os.path.exists(dir_out)\n",
        "    if not isExist:\n",
        "        os.makedirs(dir_out)    \n",
        "\n",
        "    oddRow = True\n",
        "    includeTile = True\n",
        "    listOfMainTiles = []\n",
        "    # grid = product(range(0, h-h%d, int(d/2)), range(0, w-w%d, int(d/2)))\n",
        "    grid = product(range(0, h*2-h%d, int(d/2)), range(0, w-w%d, int(d/2)))\n",
        "    for i, j in grid:\n",
        "        countW += 1\n",
        "        #print(countW)\n",
        "        box = (j, i, j+d, i+d)\n",
        "        # out = os.path.join(dir_out, f'{name}_{i}_{j}{ext}')\n",
        "        # out = os.path.join(dir_out, f'{name}_{countW}_{countH}{ext}')\n",
        "        out = os.path.join(dir_out, f'preGrid_{str(countH).zfill(3)}_{str(countW).zfill(3)}{ext}')\n",
        "        img.crop(box).save(out)\n",
        "        outputGridImagesList.append(out)\n",
        "\n",
        "        if oddRow == True and includeTile == True:\n",
        "          listOfMainTiles.append(out)\n",
        "          includeTile = False\n",
        "        else:\n",
        "          includeTile = True\n",
        "\n",
        "        if countW == cutsW:\n",
        "            countW = 0\n",
        "            countH += 1\n",
        "            if oddRow == True:\n",
        "              oddRow = False\n",
        "            else:\n",
        "              oddRow = True\n",
        "\n",
        "        if countH == (cutsH + 1):\n",
        "            output.clear()\n",
        "            image_counter +=1\n",
        "            print(\"Split: \" + str(image_counter) + \" pieces split\")\n",
        "            print(\"Finished processing! Now write descriptions for each section:\")\n",
        "            break\n",
        "      \n",
        "        output.clear()\n",
        "        image_counter +=1\n",
        "        print(\"Split: \" + str(image_counter))\n",
        "    \n",
        "    return w, h, image_counter, cutsW, outputGridImagesList, listOfMainTiles\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Core Code:\n",
        "# 1) Split the images into seperate grids which we'll feed into Stable Diffusion:\n",
        "widthAndHeight = splitImageTiles(inputImage,inputDirectory) # run the code\n",
        "initialImageWidth = (widthAndHeight[0])\n",
        "initialImageHeight = (widthAndHeight[1]) # we especially need this for later\n",
        "totalSplitCount = (widthAndHeight[2]) # to determine if we have the correct # of images\n",
        "numberOfImageAlongX = (widthAndHeight[3]) # check how many split cells go along the X axis, to check when we reach the end later\n",
        "listOfOutputImages = (widthAndHeight[4]) # the list of the output images by their path, ex: '/content/preProcessedGridSplits/preGrid_001_001.png'\n",
        "listOfMainTiles = (widthAndHeight[5]) # the list of only the main tiles by path, so we can use them to reduce prompt load by 4x if requested\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 2) Get the user to input their desired text to describe each image block:\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "imageDescriptionsList = []\n",
        "img = Image.open(os.path.join(listOfOutputImages[0]))\n",
        "time.sleep(1)\n",
        "\n",
        "if os.path.exists(\"/content/HD/text/\") == False:\n",
        "  os.makedirs(\"/content/HD/text/\")\n",
        "# note: the following will wipe out the input textfile every time we run code:\n",
        "with open('/content/HD/text/descriptionsInput.txt', 'w') as f:\n",
        "  f.write('')\n",
        "filePathTextToUse = '/content/HD/text/descriptionsInput.txt'\n",
        "img.save(\"/content/HD/_resized_Input.png\", quality=100)\n",
        "\n",
        "# we can reduce the # of prompts required to enter by 4x with the loss of accuracy:\n",
        "if reducePromptCountby4X == False:\n",
        "  countImages = totalSplitCount\n",
        "elif  reducePromptCountby4X == True:\n",
        "  countImages = int(totalSplitCount/4)\n",
        "\n",
        "\n",
        "for num in range((countImages - 1)):\n",
        "  print (\"Describing: \" + str(num+1) + \" / \" + str (countImages))\n",
        "  print (\"NOTE: If you wish to use a txt file instead of typing here, type EXIT\")\n",
        "  display(img)\n",
        "  time.sleep(0.3)\n",
        "  imageDescription = input('Enter a description for this image:\\n')\n",
        "  if imageDescription == \"EXIT\" or imageDescription == \"exit\":\n",
        "    clear_output()\n",
        "    # print (\"Cancelled typing prompt descriptions\")\n",
        "    time.sleep(0.1)\n",
        "    if reducePromptCountby4X == False:\n",
        "      print(\"Mandatory: You must include \" + str(totalSplitCount)  + \" total lines of prompts\")\n",
        "    if reducePromptCountby4X == True:\n",
        "      print(\"Mandatory: You must include \" + str(countImages) + \" total lines of prompts\")\n",
        "    differentTextFileToUse = input('Paste the full path of the text file you wish to use:\\n')\n",
        "\n",
        "    if differentTextFileToUse == \"\" or differentTextFileToUse == None:\n",
        "      differentTextFileToUse = \"/content/HD/text/descriptionsInput.txt\"\n",
        "    filePathTextToUse = differentTextFileToUse\n",
        "    break\n",
        "  else:\n",
        "    imageDescriptionsList.append(imageDescription)\n",
        "    with open('/content/HD/text/descriptionsInput.txt', 'a') as f: \n",
        "      f.write(imageDescription)\n",
        "      f.write('\\n')\n",
        "    clear_output()\n",
        "    nextImage = num+1\n",
        "    if reducePromptCountby4X == False:\n",
        "      img = Image.open(os.path.join(listOfOutputImages[nextImage]))\n",
        "    else:\n",
        "      img = Image.open(os.path.join(listOfMainTiles[nextImage]))\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    with open('/content/HD/text/descriptionsInput.txt', 'a') as f: \n",
        "      f.write(imageDescription)\n",
        "\n",
        "shutil.copy('/content/HD/text/descriptionsInput.txt', '/content/HD/text/descriptionsInput_AUTOBACKUP.txt') # Use the shutil module\n",
        "print(\"done\")\n",
        "\n",
        "# now we read the file we want to use as the text prompter and turn it into a list:\n",
        "# fileText = open(filePathTextToUse, 'r')\n",
        "with open(filePathTextToUse) as f:\n",
        "    Lines = [Line.rstrip() for Line in f]\n",
        "promptDescriptionsToAppend = Lines # here's the list of lines we'll use to go with the images\n",
        "# we'll need to duplicate these lines if we're trying to save 4x the work:\n",
        "if reducePromptCountby4X == True:\n",
        "  promptDescriptionsToAppend = []\n",
        "  LinesB = list(np.repeat(Lines, 2))\n",
        "  print(numberOfImageAlongX) # ex 12\n",
        "  counterPosOdd = 0\n",
        "  counterPosEven = 0\n",
        "  countTotal = 0\n",
        "  onOddRow = True\n",
        "  for i in range(len(LinesB)*2):\n",
        "    if onOddRow == True and countTotal == numberOfImageAlongX:\n",
        "      onOddRow = False\n",
        "      countTotal = 0\n",
        "      #print(\"odd\")\n",
        "    if onOddRow == False and countTotal == numberOfImageAlongX:\n",
        "      onOddRow = True\n",
        "      countTotal = 0   \n",
        "      #print(\"even\")\n",
        "\n",
        "    try:\n",
        "      if onOddRow == True:\n",
        "        promptDescriptionsToAppend.append(LinesB[counterPosOdd])\n",
        "        counterPosOdd +=1\n",
        "    except:\n",
        "        counterPosOdd +=1\n",
        "    try:\n",
        "      if onOddRow == False:\n",
        "        promptDescriptionsToAppend.append(LinesB[counterPosEven])\n",
        "        counterPosEven +=1\n",
        "      countTotal += 1\n",
        "    except:\n",
        "        counterPosEven +=1\n",
        "\n",
        "\n",
        "#for x in promptDescriptionsToAppend:\n",
        "#  print(x)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 3) Now we can run these prompts and images through Stable Diffusion to Upres them!\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Basic settings:\n",
        "# <br>\n",
        "#@markdown `Note: gridRatio allows you to upscale the 512 pixel tiles even more, whatever the GPU can handle. ~672 is the current cap`<br>\n",
        "batch_name = googleDriveFolderName\n",
        "gridRatio = \"672\" #@param [512, 608, 640, 672, 704, 768, 832]\n",
        "width_height = [int(gridRatio), int(gridRatio)] #param{type: 'raw'}\n",
        "guidance_scale = 20 #@param {type:\"number\"}\n",
        "steps = 40 #@param {type:\"integer\"}\n",
        "init_strength = 0.7 #@param {type:\"number\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 1 #param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 #param {type:\"number\"}\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown Prompt\n",
        "#<br>\n",
        "#@markdown `Note: leave identifiable objects out of these prompts, these should influence only style + detail`<br>\n",
        "#@markdown `These fields can be left blank if you have your full prompts saved in the text file / written in the input prompts`<br>\n",
        "promptStart = \"high resolution, close up\" #@param {type:\"string\"}\n",
        "promptEnd = \"digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\n",
        "promptList = []\n",
        "\n",
        "# append the text strings to an array to combine with full prompt later:\n",
        "for x in promptDescriptionsToAppend:\n",
        "  promptList.append(promptStart + \", \" + x + \", \" + promptEnd)\n",
        "\n",
        "# check if we actually wrote enough lines in our text file, if not we'll add empty strings:\n",
        "if len(promptList) != totalSplitCount:\n",
        "  extraLinesNeeded = totalSplitCount - len(promptList)\n",
        "  for x in range(extraLinesNeeded):\n",
        "    promptList.append(promptStart + \", \" + promptEnd)\n",
        "# ------ NOW WE HAVE OUR FULL TEXT FOR THE PROMPTS! ------------------------------\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get the input images from the folder we created earlier:\n",
        "# the folder is \"/content/HD/preProcessedGridSplits\"\n",
        "defaultInitPath = \"/content/HD/preProcessedGridSplits/\"\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(defaultInitPath):\n",
        "    if os.path.isfile(os.path.join(defaultInitPath, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "#print(numberOfImages)\n",
        "#print(init_imgs_list)\n",
        "\n",
        "# NOW WE HAVE ALL THE VARIABLES WE NEED! we have the list of images, prompts and core numbers like image height and number of tiles along width!\n",
        "# core variables:\n",
        "# promptList (lsit of the full prompts including both custom and appended style strings)\n",
        "# init_imgs_list (list of all the image paths)\n",
        "# totalSplitCount (total number of splits we made = total images to process)\n",
        "# initialImageWidth (in pixels of input image preprocessed)\n",
        "# initialImageHeight (we especially need this for later, this is the input image height in pixels)\n",
        "# numberOfImageAlongX  (check how many split cells go along the X axis)\n",
        "\n",
        "# set up counters and the output folder:\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "# numberPosition = True\n",
        "# opt.saveNumberPosition == numberPosition\n",
        "outputDirectoryProcessed = \"/content/HD/postProcessedGridSplits/\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/postProcessedGridSplits/**/*', recursive=True)\n",
        "if os.path.exists(outputDirectoryProcessed) == False:\n",
        "  os.makedirs(outputDirectoryProcessed)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get to prepare the images, prompts and settings for Stable Diffusion:\n",
        "for num in range((totalSplitCount)):\n",
        "  print(\"Processing image: \" + (str(num+1)) + \" / \" + str(totalSplitCount))\n",
        "  opt.init_img = (\"/content/HD/preProcessedGridSplits/\" + str(init_imgs_list[int(num)]))\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  # opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "  opt.outdir = outputDirectoryProcessed\n",
        "  opt.prompt = promptList[int(num)]\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_imgs_list[num],\n",
        "      'init_strength': init_strength,\n",
        "      'prompt': promptList[num],\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  #with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "  #    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "  sample_idx = 0\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images = generate(opt)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print(f\"Used seed: {opt.seed}\")\n",
        "  print(f\"Saved to: {opt.outdir}\")\n",
        "  print(\"Processed image: \" + str((num+1)) + \" / \" + str(totalSplitCount))\n",
        "  display(images[0])\n",
        "\n",
        "  batch_idx += 1\n",
        "  sample_idx += 1\n",
        "# ---------------- end of Stable Diffusion Code-----------------------\n",
        "# now some clean up before extending and merging the images:\n",
        "\n",
        "# clean up existing files again:\n",
        "finalOutputDirectory = \"/content/HD/extendedTiles/\"\n",
        "outputDirectoryFiles = glob.glob('/content/HD/extendedTiles/**/*', recursive=True)\n",
        "if os.path.exists(finalOutputDirectory) == False:\n",
        "  os.makedirs(finalOutputDirectory)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "directoryToRename = \"/content/HD/postProcessedGridSplits/\"\n",
        "directoryToReference = \"/content/HD/preProcessedGridSplits/\"\n",
        "# rename the images to clean them up:\n",
        "listOfImagesToRename = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToRename.sort()\n",
        "listOfImagesReference = [f for f in listdir(directoryToReference) if f.endswith(\".png\") and isfile(join(directoryToReference, f))]\n",
        "listOfImagesReference.sort()\n",
        "counter = 0\n",
        "for x in listOfImagesToRename:\n",
        "  newNameSplit = listOfImagesReference[counter].split('pre', 1)\n",
        "  newName = \"post\" + newNameSplit[1]\n",
        "  src =f\"{directoryToRename}/{x}\" # the incorrect file names we need to rename\n",
        "  dst =f\"{directoryToRename}/{newName}\"\n",
        "  os.rename(src, dst)\n",
        "  counter += 1\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 4) After upressing each tile with Stable Diffusion, we 'simply' expand them to full ratio and merge into a final image\n",
        "\n",
        "def add_margin(directory, columnsNumber):\n",
        "    # first gather variable information from the images in our folder:\n",
        "    listOfImages = [f for f in listdir(directory) if f.endswith(\".png\") and isfile(join(directory, f))]\n",
        "    listOfImages.sort()\n",
        "    imgStudy = Image.open(directory + '/' + listOfImages[0]) # load the image\n",
        "    w, h = imgStudy.size\n",
        "    imageWidth = w # could be 512, 640, etc, it reads the actual image width so we don't guess\n",
        "    numberOfImages = len(listOfImages) # total images in the folder we're reading\n",
        "    extendAmount = int(imageWidth / 2) # example; if the image width is 512, then this extend amount is 256\n",
        "    # extensionWidth = int(imageWidth * (columnsNumber / 2)) # the final image width, example 2048\n",
        "    # extensionHeight = int((numberOfImages / (columnsNumber)) * extendAmount) # the final image height, example 3072\n",
        "    \n",
        "    # these values come from the actual image, so it's more legit:\n",
        "    # since we have a feature called gridRatio which actually changes the final width, we'll have to accomodate new potential sizes:\n",
        "    if imageWidth == 512:\n",
        "      extensionWidth = initialImageWidth\n",
        "      extensionHeight = initialImageHeight\n",
        "    else:\n",
        "      pixelDensityIncrease = imageWidth / 512\n",
        "      extensionWidth = int(initialImageWidth * pixelDensityIncrease)\n",
        "      extensionHeight = int(initialImageHeight * pixelDensityIncrease) \n",
        "\n",
        "    # print(extensionHeight)\n",
        "\n",
        "    # next create variables for extending / adding transparency to the images:\n",
        "    color =  (0, 0, 0, 0) # the last number, 0 means the value of the extension will be completely transparent if we save as .png\n",
        "    extendTop = 0 # we start at zero since we start from the top left corner\n",
        "    extendBottom = int((extensionHeight - imageWidth) + imageWidth) # we start with a value since there is space on the bottom in the 1st image; example 2560\n",
        "    extendLeft = 0 # we start at zero since we start from the top left corner\n",
        "    extendLeft = int((imageWidth / 2) - imageWidth) # we actually start in the negative since we begin by adding + 256 to the left, which brings us to 0 initially\n",
        "    extendRight = int(extensionWidth +  (imageWidth/2)) # we start with a value since there is space on the right in the 1st image; example 2304, then subtract 256\n",
        "\n",
        "    # we have to check which image we're on, both in total, by column and by row:\n",
        "    image_counter = 0\n",
        "    countW = 1\n",
        "    countH = 0\n",
        "\n",
        "    # these values controls how the mask looks and we also need to gather the resulting img + mask for when we stitch them back together:\n",
        "    maskBorderAmount = 96\n",
        "    maskBlurIntensity = 16\n",
        "    listOfExportedImages = []\n",
        "    listOFExportedImageMasks = []\n",
        "\n",
        "    # before processing the images, check if a folder to store the new images exists:\n",
        "    # filePathSave = directory + \"\\\\\" + \"extendedFrame\" + \"\\\\\"\n",
        "    filePathSave = \"/content/HD/extendedTiles/\"\n",
        "    isExist = os.path.exists(filePathSave)\n",
        "    if not isExist:\n",
        "        os.makedirs(filePathSave)    \n",
        "\n",
        "    # now cycle through the images and extend them, etc:\n",
        "    for image in listOfImages:\n",
        "        # read and number the image:\n",
        "        image_counter += 1\n",
        "        image_counterPadded = str(image_counter).zfill(4) # the number we label each image is padded; example 0001\n",
        "        img = Image.open(directory + '/' + image) # load the image\n",
        "        img = img.convert(\"RGBA\") # make sure this image supports transparency\n",
        "\n",
        "        # calculate how much to extend the canvas in all directions:\n",
        "        if countW <= columnsNumber: # if the count is lower than the number of images width wise, we only need to extend on the width \n",
        "            countW += 1\n",
        "            extendLeft += int(imageWidth / 2) # example: 256\n",
        "            extendRight -= int(imageWidth / 2)\n",
        "            # print(\"exension amount left: \" + str(extendLeft))\n",
        "        elif countW > columnsNumber:\n",
        "            # once we exceed the number of images width wise, we reset the count/width and change the spacing just once for the height\n",
        "            countW = 2\n",
        "            countH +=1\n",
        "            extendTop += int(imageWidth / 2) # example: 256\n",
        "            extendBottom -= int(imageWidth / 2)\n",
        "            extendLeft = 0\n",
        "            extendRight = extensionWidth\n",
        "\n",
        "        # finally calculate how to edit the dimensions of the transparent bits of the image:\n",
        "        # width, height = img.size\n",
        "        new_width = extendRight + extendLeft\n",
        "        new_height = extendTop + extendBottom\n",
        "        result = Image.new(img.mode, (new_width, new_height), color)\n",
        "        result.paste(img, (extendLeft, extendTop))\n",
        "\n",
        "        # count which actual column and row we're on:\n",
        "        image_rowPadded = str(countH+1).zfill(3)\n",
        "        image_columnPadded = str(countW-1).zfill(3)\n",
        "\n",
        "        # create a feather mask around the cropped image to blend it in better when compositing:\n",
        "        mask_im = Image.new(\"L\", result.size, 0)\n",
        "\n",
        "        draw = ImageDraw.Draw(mask_im)\n",
        "        # draw.rectangle((extendLeft+ 12, extendTop + 12, extendRight - (extendRight-imageWidth) -18, extendBottom - (extendBottom - imageWidth)-18), fill=255)\n",
        "        #maskX = int((imageWidth/2) * (countW - 2) + maskBorderAmount)\n",
        "        #maskY = int((imageWidth/2) * (countH) + maskBorderAmount)\n",
        "        if countW == 2:\n",
        "            maskX = int((imageWidth/2) * (countW - 2))\n",
        "        else:\n",
        "            maskX = int((imageWidth/2) * (countW - 2) + maskBorderAmount)\n",
        "        if countH == 0:\n",
        "            maskY = int((imageWidth/2) * (countH))\n",
        "        else:\n",
        "            maskY = int((imageWidth/2) * (countH) + maskBorderAmount)\n",
        "        maskW = int(((imageWidth/2) * (countW - 2)) + imageWidth - maskBorderAmount)\n",
        "        maskH = int(((imageWidth/2) * (countH)) + imageWidth - maskBorderAmount)\n",
        "        draw.rectangle((maskX, maskY, maskW, maskH), fill=255) # THIS is where we draw the MASK, fill=255 must mean it's a perfect white? I forgot\n",
        "        # create a blur for the mask:\n",
        "        mask_im_blur = mask_im.filter(ImageFilter.GaussianBlur(maskBlurIntensity)) # maskBlurIntensity is the variable we define to control how intense to mask\n",
        "\n",
        "        # finally, save this image in a new folder for the extended canvas version:\n",
        "        # filePathSaveFull = filePathSave + \"paddedImage\" + \"_\" + image_counterPadded + \".png\"\n",
        "        filePathSaveFull = filePathSave + \"paddedImage\" + \"_\" + image_rowPadded + \"_\" + image_columnPadded + \".png\"\n",
        "        result.save(filePathSaveFull, quality=100)\n",
        "        listOfExportedImages.append(result)\n",
        "        # save the mask too (actually I rather not save the mask, these are easy to make in Photoshop and this saves time not saving):\n",
        "        # filePathSaveFull = filePathSave + \"mask\" + \"_\" + image_counterPadded + \".png\"\n",
        "        # mask_im_blur.save(filePathSaveFull, quality=100)\n",
        "        listOFExportedImageMasks.append(mask_im_blur) # instead of saving the masks, save them to a variable\n",
        "        output.clear()\n",
        "        print(\"Extended: \" + str(image_counter) + \" / \" + str(len(listOfImages)))\n",
        "\n",
        "    # after extending the frames and creating the masks, combine them into a single image:\n",
        "    back_img = listOfExportedImages[0].copy()\n",
        "    countMask = 0\n",
        "    filePathSaveFull = \"/content/HD/\" + \"_final_HD\" + \".png\"\n",
        "\n",
        "    for image in listOfExportedImages:\n",
        "        #listOfExportedImages = []\n",
        "        #listOFExportedImageMasks = []\n",
        "        if image == listOfExportedImages[0]:\n",
        "            back_img.save(filePathSaveFull, quality=100)\n",
        "        else:\n",
        "            #back_img = Image.open(filePathSaveFull) # load the image\n",
        "            back_img.paste(image, (0,0), listOFExportedImageMasks[countMask])\n",
        "            #back_img.save(filePathSaveFull, quality=100)\n",
        "        countMask += 1\n",
        "        output.clear()\n",
        "        # Preview image in UI:\n",
        "        basewidth = 280\n",
        "        wpercent = (basewidth / float(back_img.size[0]))\n",
        "        hsize = int((float(back_img.size[1]) * float(wpercent)))\n",
        "        imgResized = back_img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
        "        display(imgResized)\n",
        "        time. sleep(0.6)\n",
        "        print(\"Processed: \" + str(countMask) + \" / \" + str(len(listOfExportedImages)))\n",
        "    back_img.save(filePathSaveFull, quality=100)\n",
        "    output.clear()\n",
        "    # Preview image in UI:\n",
        "    basewidth = 512\n",
        "    wpercent = (basewidth / float(back_img.size[0]))\n",
        "    hsize = int((float(back_img.size[1]) * float(wpercent)))\n",
        "    imgResized = back_img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
        "    display(imgResized)\n",
        "    print(\"done\")\n",
        "\n",
        "# img_new = add_margin(r\"H:\\New\\testUpres\\testCodeHD\", 512, 8)\n",
        "img_new = add_margin(r\"/content/HD/postProcessedGridSplits\", numberOfImageAlongX) # you need to include input folder, make sure only the # of images you want are in there and include # of total images in width\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Now we copy the final files to Google Drive to back them up:\n",
        "print(\"Wait while copying to Google Drive...\")\n",
        "import shutil\n",
        "copyAddressOnDrive = \"/content/gdrive/MyDrive/AI/StableDiffusionHD/\" + batch_name\n",
        "\n",
        "directoryToCopy = \"/content/HD/extendedTiles/\"\n",
        "listOfImagesToCopy = [f for f in listdir(directoryToCopy) if f.endswith(\".png\") and isfile(join(directoryToCopy, f))]\n",
        "listOfImagesToCopy.sort()\n",
        "\n",
        "isExist = os.path.exists(copyAddressOnDrive + \"/tiles/\")\n",
        "if not isExist:\n",
        "    os.makedirs(copyAddressOnDrive + \"/tiles/\")\n",
        "\n",
        "shutil.copy(\"/content/HD/_final_HD.png\", copyAddressOnDrive+\"/\")\n",
        "shutil.copy(\"/content/HD/_resized_Input.png\", copyAddressOnDrive+\"/\")\n",
        "print(\"Done copying files. Enjoy!\")\n",
        "\n",
        "for x in listOfImagesToCopy:\n",
        "  fullPath = directoryToCopy + x\n",
        "  shutil.copy(fullPath, copyAddressOnDrive + \"/tiles/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grid Preview before upscaling *(optional)*:\n",
        "#@markdown `Note: This upscaling tool will always generate each tile from 512x512 sections`<br>\n",
        "#@markdown `Turn on adjustSize and play with the widthAdjustment until you're happy`<br>\n",
        "#@markdown `If you want to use the same ratio in the main tool, edit the image externally or use the same widthAdjustment there as well`<br>\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import PIL\n",
        "import math\n",
        "\n",
        "counter = 0\n",
        "selectInitialImage = 0\n",
        "countRepeatPrompts = 0\n",
        "tempCollectExtendedImg = []\n",
        "imgPathOutputFin = \"\"\n",
        "\n",
        "imageSelected = \"/content/init_image/start.png\" #@param{type:\"string\"}\n",
        "\n",
        "imgMain = Image.open(imageSelected) \n",
        "w, h = imgMain.size\n",
        "\n",
        "adjustSize = True #@param{type:\"boolean\"}\n",
        "widthAdjustment = 2048 #@param{type:\"integer\"}\n",
        "if adjustSize == True:\n",
        "  wpercent = (widthAdjustment / float(imgMain.size[0]))\n",
        "  hsize = int((float(imgMain.size[1]) * float(wpercent)))\n",
        "  imgResized = imgMain.resize((widthAdjustment, hsize), Image.ANTIALIAS)\n",
        "  w, h = imgResized.size\n",
        "  imgMain = imgResized\n",
        "\n",
        "# imagePlainBG = imgMain.copy()\n",
        "imagePlainBG = PIL.Image.new(mode = \"RGBA\", size = (w,h), color = (0, 0, 0, 255))\n",
        "imagePlainBGWhite = PIL.Image.new(mode = \"RGBA\", size = (w,h), color = (255, 255, 255))\n",
        "\n",
        "imgGrid = PIL.Image.new(mode = \"RGBA\", size = (512,512), color = (200, 255, 255, 255))\n",
        "\n",
        "imgMaskOutline = ImageDraw.Draw(imgGrid)  \n",
        "thicknessLine = int(3)\n",
        "shape = [(thicknessLine, thicknessLine), (512 - thicknessLine, 512 - thicknessLine)]\n",
        "imgMaskOutline.rectangle(shape, fill =(0, 0, 0, 0)) #blackfill\n",
        "# display(imgGrid)\n",
        "\n",
        "numberOfTilesWidth = math.ceil(w / 512)\n",
        "numberOfTilesHeight = math.ceil(h / 512)\n",
        "\n",
        "totalTiles = numberOfTilesWidth * numberOfTilesHeight\n",
        "\n",
        "countw=0\n",
        "counth=0\n",
        "positionW = 0\n",
        "positionH = 0\n",
        "\n",
        "for tile in range(totalTiles):\n",
        "  if countw == numberOfTilesWidth:\n",
        "    counth+=1\n",
        "    countw=0\n",
        "  positionW = countw * 512\n",
        "  positionH = counth * 512  \n",
        "  imagePlainBG.paste(imgGrid, (positionW, positionH))\n",
        "  countw+=1\n",
        "\n",
        "# convert from RGB to RGBA to use the grid as a mask:\n",
        "img_rgba = imagePlainBG.convert(\"RGBA\")\n",
        "#img_rgba.mode\n",
        "\n",
        "imgMainWithGrid = imgMain.copy()\n",
        "imgMainWithGrid.paste(imagePlainBGWhite, (0,0), img_rgba)\n",
        "\n",
        "# resize the image to fit better in the preview:\n",
        "basewidth = 512\n",
        "wpercent = (basewidth / float(imgMainWithGrid.size[0]))\n",
        "hsize = int((float(imgMainWithGrid.size[1]) * float(wpercent)))\n",
        "imgResized = imgMainWithGrid.resize((basewidth, hsize), Image.ANTIALIAS)\n",
        "totalPrompts = totalTiles*4\n",
        "\n",
        "print(\"Total Tiles:\",totalTiles, \"tiles =\", totalTiles,\"-\",totalPrompts, \"prompts\")\n",
        "display(imgResized)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VK5RjrY8DjG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copy preProcessedGridSplits tiles to Google Drive *(optional)*:\n",
        "#@markdown `These tiles are the ones cropped before any Stable Diffusion is done to them, in case you want to run them through img2img at a later point`<br>\n",
        "#@markdown `It will also save the images in a numeric folder so you won't accidentally overwrite any files. Just clean up your folders after`<br>\n",
        "\n",
        "\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "\n",
        "copyAddressOnDrive = \"/content/gdrive/MyDrive/AI/StableDiffusionHD\" + \"/preProcessedGrids/\"\n",
        "\n",
        "directoryToCopy = \"/content/HD/preProcessedGridSplits/\"\n",
        "listOfImagesToCopy = [f for f in listdir(directoryToCopy) if f.endswith(\".png\") and isfile(join(directoryToCopy, f))]\n",
        "listOfImagesToCopy.sort()\n",
        "\n",
        "isExist = os.path.exists(copyAddressOnDrive + \"/001\")\n",
        "if not isExist:\n",
        "    os.makedirs(copyAddressOnDrive + \"/001\")\n",
        "    copyAddressOnDrive = copyAddressOnDrive + \"/001\"\n",
        "else:\n",
        "  i=1\n",
        "  while os.path.exists(copyAddressOnDrive + \"/\" + str(i).zfill(3)):\n",
        "    i+=1\n",
        "  os.makedirs(copyAddressOnDrive + \"/\" + str(i).zfill(3))\n",
        "  copyAddressOnDrive = copyAddressOnDrive + \"/\" + str(i).zfill(3)\n",
        "\n",
        "for x in listOfImagesToCopy:\n",
        "  fullPath = directoryToCopy + x\n",
        "  shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "print(\"Copied tiles to: \" + copyAddressOnDrive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "jYhOWi_IUzG1",
        "outputId": "aaa4312d-bf3c-47df-e4cb-c995aeb6f1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied tiles to: /content/gdrive/MyDrive/AI/StableDiffusionHD/preProcessedGrids//005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCQrrKma9HA"
      },
      "source": [
        "## <u> B) Post Processing - **Face Recognition Upscaling** </u> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PvTzaOl9pw4W"
      },
      "outputs": [],
      "source": [
        "#@title Face Recognition & Upscale (Run):\n",
        "#@markdown `This script will process all the images in the imgPath location, sequentially by name`<br>\n",
        "#@markdown `Turn on useRealESRGANforUpscale for higher quality results, just takes longer`<br>\n",
        "#@markdown `If it fails, the ESRGAN scale may be too high, your input image is too high res or the grid ratio is too high`<br>\n",
        "#@markdown `Predictable low end results: input images size 640x640, scale factor 2, grid ratio 672, init_strength 0.5`<br>\n",
        "\n",
        "##@markdown\n",
        "\n",
        "import re\n",
        "import random\n",
        "# facial recognition code:\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from base64 import b64decode\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# Create the folders & folder variables we need: -----------\n",
        "ProjectName = \"testFaceRecognitionF\" #@param {type:\"string\"}\n",
        "# the folder to contain all the other folders:\n",
        "FRpathBase = \"/content/facialRecognition/\"\n",
        "os.makedirs(FRpathBase, exist_ok=True) \n",
        "# made a project for everything as well, why not:\n",
        "FRpathProject = \"/content/facialRecognition/\" + ProjectName + \"/\"\n",
        "os.makedirs(FRpathProject, exist_ok=True) \n",
        "# where the study material (ex previewing how the crops will look) are saved\n",
        "FRpathProjectWIP = \"/content/facialRecognition/\" + ProjectName + \"/workFiles/\"\n",
        "os.makedirs(FRpathProjectWIP, exist_ok=True) \n",
        "# where the pre-gen crops will be saved:\n",
        "FRpathProjectCrops = \"/content/facialRecognition/\" + ProjectName + \"/crops/\"\n",
        "os.makedirs(FRpathProjectCrops, exist_ok=True) \n",
        "# where the newly generated but still cropped faces will be saved:\n",
        "FRpathProjectUprezzedCrops = \"/content/facialRecognition/\" + ProjectName + \"/uprezzedCrops/\"\n",
        "os.makedirs(FRpathProjectCrops, exist_ok=True) \n",
        "# where the final upscaled and face swapped images will be saved:\n",
        "FRpathProjectFinal = \"/content/facialRecognition/\" + ProjectName + \"/finals/\"\n",
        "os.makedirs(FRpathProjectFinal, exist_ok=True) \n",
        "# where the real-ESRGAN upscaled images will be temp saved:\n",
        "FRpathProjectUpscaledINIT = \"/content/facialRecognition/\" + ProjectName + \"/upscaledINIT/\"\n",
        "os.makedirs(FRpathProjectUpscaledINIT, exist_ok=True) \n",
        "# where the extended transparent results are stored:\n",
        "FRpathProjectExtended = \"/content/facialRecognition/\" + ProjectName + \"/extended/\"\n",
        "os.makedirs(FRpathProjectExtended, exist_ok=True) \n",
        "\n",
        "# clean up the work files (only clean up the base directory):\n",
        "outputDirectoryProcessed = FRpathBase\n",
        "outputDirectoryFiles = glob.glob(FRpathBase + '/**/*', recursive=True)\n",
        "if os.path.exists(outputDirectoryProcessed) == False:\n",
        "  os.makedirs(outputDirectoryProcessed)\n",
        "# clean up existing files in the output Directory:\n",
        "for f in outputDirectoryFiles:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        pass\n",
        "\n",
        "# save the positions to lists to call later:\n",
        "countOfFacesPerInput = []\n",
        "positionFaceImageXList = []\n",
        "positionFaceImageYList = []\n",
        "positionFaceImageWList = []\n",
        "positionFaceImageHList = []\n",
        "expandFacialRecognitionRatio = 15 \n",
        "\n",
        "# A) Check the Image for Faces and crop it: -------------------------------------------\n",
        "imgPath = \"/content/init_image\" #@param {type:\"string\"}\n",
        "imgPath = imgPath + \"/\"\n",
        "imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "imgPathInput.sort() # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "numberOfImagesToProcess = len(imgPathInput)\n",
        "counter = 0\n",
        "#@markdown `Turn on ESRGAN for the input images only if they're low res`<br>\n",
        "useRealESRGANforUpscale = True #@param {type:\"boolean\"}\n",
        "ESRGANscaleFactor = 2 #@param {type:\"slider\", min:2, max:4, step:0.5}\n",
        "#@markdown `for faceRecognitionMinimizer, higher values = less faces detected. Default is 3-5`<br>\n",
        "faceRecognitionMinimizer = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Real-ESRGAN Upscaling for the Input Images:\n",
        "# since it takes a while to upscale, make this feature optional:\n",
        "if useRealESRGANforUpscale == True:\n",
        "  # it seems only png work, so convert .jpg to PNG:\n",
        "  for img in imgPathInput:\n",
        "    if img.endswith('.jpg'):\n",
        "      im1 = Image.open(imgPath + img)\n",
        "      splitPath = os.path.splitext(imgPath + img)[0]\n",
        "      newPath = splitPath + '.png'\n",
        "      im1.save(newPath)\n",
        "      os.remove(imgPath + img)\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort() # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "\n",
        "  #1) set up the file to load into the ESRGAN:\n",
        "  # note: these folders live inside the real ESRGAN created folders, which is why they do not have /content/ in their name\n",
        "  upload_folder = 'upload' # wierd directories that ESRGAN uses\n",
        "  result_folder = 'results'# ignore the lack of /content/ directory\n",
        "\n",
        "  # create the upload and results folders + clean them up:\n",
        "  if os.path.isdir(upload_folder):\n",
        "      shutil.rmtree(upload_folder)\n",
        "  if os.path.isdir(result_folder):\n",
        "      shutil.rmtree(result_folder)\n",
        "  os.mkdir(upload_folder)\n",
        "  os.mkdir(result_folder)\n",
        "\n",
        "  # upload images: (in this case we read our init_images)\n",
        "  uploadPath = imgPath\n",
        "  filesList = imgPathInput\n",
        "  for filename in filesList:\n",
        "    dst_path = os.path.join(upload_folder, filename)\n",
        "    print(f'copy {filename} to {dst_path}')\n",
        "    shutil.copy(uploadPath + filename, dst_path)\n",
        "\n",
        "  # 2) Core Code to Make ESRGAN Uprez the Image:\n",
        "  # if it is out of memory, try to use the `--tile` option\n",
        "  # We upsample the image with the scale factor X3.5\n",
        "  if ESRGANscaleFactor == 2:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2 --face_enhance\n",
        "  elif ESRGANscaleFactor == 2.5:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2.5 --face_enhance  \n",
        "  elif ESRGANscaleFactor == 3:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3 --face_enhance\n",
        "  elif ESRGANscaleFactor == 3.5:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3.5 --face_enhance\n",
        "  elif ESRGANscaleFactor == 4:\n",
        "    !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 4 --face_enhance\n",
        "  # Arguments\n",
        "  # -n, --model_name: Model names\n",
        "  # -i, --input: input folder or image\n",
        "  # --outscale: Output scale, can be arbitrary scale factore. \n",
        "  input_folder = '/content/stable-diffusion/Real-ESRGAN/upload'\n",
        "  result_folder = '/content/stable-diffusion/Real-ESRGAN/results'\n",
        "  input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "  output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "  output_list.sort()\n",
        "  #print(input_list)\n",
        "  #print(output_list) # list of upressed files\n",
        "\n",
        "  # now we replace the original variables with our new images:\n",
        "  imgPath = result_folder + \"/\"\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "  time.sleep(1)\n",
        "  # copy the files to easer to see location:\n",
        "  for item in imgPathInput:\n",
        "    fullPath = imgPath + item\n",
        "    shutil.copy(fullPath, FRpathProjectUpscaledINIT)\n",
        "\n",
        "  imgPath = FRpathProjectUpscaledINIT\n",
        "  imgPathInput = [f for f in listdir(imgPath) if isfile(join(imgPath, f))]\n",
        "  imgPathInput.sort() # this variable gives a list of all the images in the init_image folder (ex. [img1.png, img2.png])\n",
        "# print(\"imgPathInput:\" + str(imgPathInput))\n",
        "  # ------- END real ESRGAN -----------------\n",
        "\n",
        "\n",
        "\n",
        "checkIfRealFace = 0\n",
        "# -------------------------------------------------------------------------------\n",
        "# -------------------------FACIAL RECOGNITION: ----------------------------------\n",
        "# A) Check the Image for Faces and crop it: -------------------------------------------\n",
        "for image in imgPathInput:\n",
        "  counter +=1\n",
        "  counterString = str(counter)\n",
        "  inputImageFullPath = imgPath + image\n",
        "  imgPathOutput = FRpathProjectWIP + \"study_\" + counterString.zfill(4) + \".png\" # ex. study1.png\n",
        "  # print(imgPathInput)\n",
        "  # we need to save the x and y coordinates of the face image so we can swap it out later:\n",
        "  # we also need to save the length and width:\n",
        "\n",
        "  positionFaceImageX = 0\n",
        "  positionFaceImageY = 0\n",
        "  positionFaceImageW = 0\n",
        "  positionFaceImageH = 0\n",
        "\n",
        "  # 1) read the image from path:\n",
        "  img = cv2.imread(inputImageFullPath)\n",
        "  imgCopy = img.copy() # we create a copy of the input image to purform the cropping on since we use the original link to create the study images\n",
        "  fullImageHeight, fullImageWidth, channels  = img.shape # we need to know the full image width to know the ratio to expand the recognition\n",
        "\n",
        "  # 2) convert image to grayscale (required by image recognition model):\n",
        "  gray_img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # 3) Use the Haar Cascade model to detect faces from the image:\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "  # it seems higher resolution images requires us to modify the intensity of searching for faces, which is this value:\n",
        "  if useRealESRGANforUpscale == False:\n",
        "    faces = face_cascade.detectMultiScale(gray_img,1.1,faceRecognitionMinimizer)\n",
        "  elif useRealESRGANforUpscale == True:\n",
        "    faces = face_cascade.detectMultiScale(gray_img,1.1,faceRecognitionMinimizer*2)\n",
        "\n",
        "  # 4) print the number of faces detected:\n",
        "  #print(f\"len(faces)} faces detected in the image.\")\n",
        "  numberOfFacesDetected = len(faces)\n",
        "  counterFaces = 0\n",
        "  countOfFacesPerInput.append(numberOfFacesDetected)\n",
        "  print(\"Number of faces detected:\" + str(numberOfFacesDetected))\n",
        "\n",
        "  # 5a) Draw rectangle around the faces and crop the faces:\n",
        "  # 5b) also save the correct position data (the distance of top left corner + crop W & H):\n",
        "  for f in faces:\n",
        "    counterFaces +=1\n",
        "    counterFacesString = str(counterFaces)\n",
        "    x1, y1, w1, h1 = [ v for v in f ]\n",
        "    calculateDistancePixelsPercentage = int(fullImageWidth / expandFacialRecognitionRatio)\n",
        "    calculateDistancePixels = int(calculateDistancePixelsPercentage)\n",
        "    positionFaceImageX = x = int(x1 - calculateDistancePixels)\n",
        "    positionFaceImageY = y = int(y1 - calculateDistancePixels)\n",
        "    positionFaceImageW = w = int(w1 + (calculateDistancePixels * 2))\n",
        "    positionFaceImageH = h = int(h1 + (calculateDistancePixels * 2))\n",
        "    positionFaceImageXList.append(positionFaceImageX)\n",
        "    positionFaceImageYList.append(positionFaceImageY)\n",
        "    positionFaceImageWList.append(positionFaceImageW)\n",
        "    positionFaceImageHList.append(positionFaceImageH)\n",
        "\n",
        "    sub_face = imgCopy[y:y+h, x:x+w]\n",
        "\n",
        "    fname, ext = os.path.splitext(inputImageFullPath)\n",
        "    # cv2.rectangle(img, (x,y), (x+w,y+h), (256,256,256), thickness=20) # FOR DEBUGGING IF THE IMAGE IS BEING PLACED CORRECTLY BACK ON THE FULL IMG\n",
        "    #cv2.imwrite(\"/content/known/\" + fname+\"_cropped_\"+ext, sub_face)\n",
        "    #cv2.imwrite(FRpathProjectCrops + \"cropped\" + \".png\", sub_face)\n",
        "    try:\n",
        "      cv2.imwrite(FRpathProjectCrops + \"cropped_\" + counterString.zfill(4) + \"_f\" + counterFacesString.zfill(2) + \".png\", sub_face)\n",
        "      cv2.rectangle(img, (x,y), (x+w,y+h), (256,256,256), thickness=2)\n",
        "      cv2.rectangle(img, (x1, y1), (x1 + w1, y1+h1), color=(0, 256, 0), thickness=1) \n",
        "      #break # only process the first face result, if there are multiple faces those get skipped\n",
        "      checkIfRealFace += 1\n",
        "    except:\n",
        "      \"not a face!\"\n",
        "\n",
        "  # save the image with rectangles\n",
        "  cv2.imwrite(imgPathOutput, img) # the study image\n",
        "\n",
        "# since we may have more than one face in the images, calculate the total # of images overall:\n",
        "numberOfFacesDetected = checkIfRealFace\n",
        "\n",
        "totalNumberOFCrops = 0\n",
        "for x in countOfFacesPerInput:\n",
        "  totalNumberOFCrops += x\n",
        "\n",
        "# ----------------------END FACIAL RECOGNITION P1 -------------------------------\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# B) Run Stable Diffusion to Process Newly Generated Image:\n",
        "\n",
        "\n",
        "#B1) first deal with getting the input images and the prompts:\n",
        "promptList = []\n",
        "promptList = [\"an attract female portrait by artgerm, greg rutkowski, trending on artstation, detailed, rendered in Octane, ethereal\"]\n",
        "\n",
        "# create a list of all the prompts from new line:\n",
        "#if type(promptsListForOpenCV) is str:\n",
        "#  promptsListForOpenCV = [i for i in promptsListForOpenCV.split('\\n') if i]\n",
        "\n",
        "# UPDATE: 07/09/2022:\n",
        "# I added the prompts to a text file to make it more user friendly to deal with them, so this is now how they're loaded:\n",
        "with open('/content/promptTexts/_promptsMain.txt') as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "promptsA = lines\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 3) Now we can run these prompts and images through Stable Diffusion to Upres them!\n",
        "\n",
        "\n",
        "#@markdown Basic settings:\n",
        "batch_name = ProjectName\n",
        "gridRatio = \"640\" #@param [512, 608, 640, 672, 704]\n",
        "width_height = [int(gridRatio), int(gridRatio)] #param{type: 'raw'}\n",
        "guidance_scale = 20 #@param {type:\"number\"}\n",
        "steps = 60 #@param {type:\"integer\"}\n",
        "init_strength = 0.7 #@param {type:\"number\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "num_batch_images = 1 #param {type:\"integer\"}\n",
        "sampler = 'ddim' #@param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 #param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"integer\"}\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get the input images from the folder we created earlier:\n",
        "\n",
        "# find all the images in the folder meant to read them:\n",
        "numberOfImages = 0\n",
        "init_imgs_list = []\n",
        "for path in os.listdir(FRpathProjectCrops): # note: imgPath is defined as a parameter at the beginning of the code\n",
        "    if os.path.isfile(os.path.join(FRpathProjectCrops, path)):\n",
        "        numberOfImages += 1\n",
        "        init_imgs_list.append(path)\n",
        "init_imgs_list.sort()\n",
        "#print(numberOfImages)\n",
        "#print(init_imgs_list)\n",
        "\n",
        "# --------------- Next we deal with Prompts: -------------\n",
        "#@markdown Prompt\n",
        "promptStart = \"female\" #@param {type:\"string\"}\n",
        "promptEnd = \"portrait, young woman, detailed gorgeous face, digital art, painting, artstation, concept art, smooth, sharp focus, high definition, detailed, illustration, art by artgerm and greg rutkowski and alphonse mucha\" #@param {type:\"string\"}\n",
        "promptList = []\n",
        "promptListFullString = []\n",
        "\n",
        "# append the text strings to an array to combine with full prompt later:\n",
        "for x in promptsA:\n",
        "  # this one will just have the prompts we have written + the prompt template, it will not duplicate prompts over multi face images yet\n",
        "  promptListFullString.append(promptStart + \", \" + x + \", \" + promptEnd)\n",
        "\n",
        "# check if we actually wrote enough lines in our text file, if not we'll add empty strings:\n",
        "if len(promptListFullString) != numberOfImagesToProcess: # number of images to process is the total number of INIT images\n",
        "  extraLinesNeeded = numberOfImagesToProcess - len(promptListFullString)\n",
        "  for x in range(extraLinesNeeded):\n",
        "    promptListFullString.append(promptStart + \", \" + promptEnd)\n",
        "\n",
        "counter = 0\n",
        "# now we check how many of these lines we need to duplicate for multi face images:\n",
        "for x in countOfFacesPerInput: # this tells us how many faces are in per image; ex [0, 1, 6]\n",
        "  if x == 0:\n",
        "    # if there are no faces in the image, skip appending that line\n",
        "    counter += 1\n",
        "  else:\n",
        "    for y in range (x):\n",
        "      promptList.append(promptListFullString[counter]) # append this line as many times as there are faces in the image\n",
        "    counter += 1 # after appending all the neccessary lines, move on to the next line\n",
        "\n",
        "#print(len(promptList))\n",
        "#for x in promptList:\n",
        "#  print(x)\n",
        "# ------ NOW WE HAVE OUR FULL TEXT FOR THE PROMPTS! ----------------------------\n",
        "#print(len(init_imgs_list))\n",
        "#print(init_imgs_list)\n",
        "\n",
        "\n",
        "\n",
        "# set up counters and the output folder:\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "# numberPosition = True\n",
        "# opt.saveNumberPosition == numberPosition\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# now get to prepare the images, prompts and settings for Stable Diffusion:\n",
        "for num in range((totalNumberOFCrops)):\n",
        "  print(\"Processing image: \" + (str(num+1)) + \" / \" + str(totalNumberOFCrops))\n",
        "  if len(init_imgs_list) == 0:\n",
        "    print(\"No faces detected! Exiting\")\n",
        "    break\n",
        "  opt.init_img = (FRpathProjectCrops + str(init_imgs_list[int(num)]))\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  # opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "  opt.outdir = FRpathProjectUprezzedCrops\n",
        "  opt.prompt = promptList[int(num)]\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W, opt.H = map(lambda x: x - x % 64, (width_height[0], width_height[1])) # resize to integer multiple of 64\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_imgs_list[num],\n",
        "      'init_strength': init_strength,\n",
        "      'prompt': promptList[num],\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  #with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "  #    json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "  sample_idx = 0\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images = generate(opt)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print(f\"Used seed: {opt.seed}\")\n",
        "  print(f\"Saved to: {opt.outdir}\")\n",
        "  print(\"Processed image: \" + str((num+1)) + \" / \" + str(totalNumberOFCrops))\n",
        "  display(images[0])\n",
        "\n",
        "  batch_idx += 1\n",
        "  sample_idx += 1\n",
        "\n",
        "# ------- END OF STABLE DIFFUSION PROCESSING ------------------\n",
        "\n",
        "# Clean up the files in /uprezzedCrops/\n",
        "# now some clean up before merging the cropped face images into the original images:\n",
        "\n",
        "# FRpathProjectFinal # /content/facialRecognition/testFacialRecognition/finals\n",
        "\n",
        "directoryToRename = FRpathProjectUprezzedCrops # /content/facialRecognition/testFacialRecognition/uprezzedCrops\n",
        "# rename the images to clean them up:\n",
        "listOfImagesToRename = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToRename.sort()\n",
        "counter = 1\n",
        "listOfImagesToIntegrate = []\n",
        "for x in listOfImagesToRename:\n",
        "  counterString = str(counter)\n",
        "  # newName = \"result_\" + counterString.zfill(4) + \".png\"\n",
        "  newName = \"result_\" + counterString.zfill(4) + \".png\"\n",
        "  src =f\"{directoryToRename}/{x}\" # the incorrect file names we need to rename\n",
        "  dst =f\"{directoryToRename}/{newName}\"\n",
        "  os.rename(src, dst)\n",
        "  counter += 1\n",
        "\n",
        "listOfImagesToIntegrate = [f for f in listdir(directoryToRename) if f.endswith(\".png\") and isfile(join(directoryToRename, f))]\n",
        "listOfImagesToIntegrate.sort()\n",
        "# end clean up\n",
        "\n",
        "\n",
        "# ------- RESIZING AND STITCHING CODE TO FINISH IT OFF: ---------------\n",
        "counter = 0\n",
        "selectInitialImage = 0\n",
        "countRepeatPrompts = 0\n",
        "tempCollectExtendedImg = []\n",
        "imgPathOutputFin = \"\"\n",
        "for image in listOfImagesToIntegrate:\n",
        "  # RESIZE IMAGES: -------------------------------------------\n",
        "  # since we generate the close up crops in a larger resolution, we have to unify them with the original images\n",
        "\n",
        "  # first gather information from the images in our folders:\n",
        "  imgSizeCropped = Image.open(FRpathProjectCrops + '/' + init_imgs_list[counter]) # load the image from /crops/\n",
        "  w, h = imgSizeCropped.size # shoud be on the small side\n",
        "  imageWidthCropped = w\n",
        "\n",
        "  imgSizeGenerated = Image.open(FRpathProjectUprezzedCrops + '/' + listOfImagesToIntegrate[counter]) # load the image from /uprezzedCrops/\n",
        "  w, h = imgSizeGenerated.size # should be 512 pixels +\n",
        "  imageWidthGenerated = w # could be 512, 640, etc, it reads the actual image width so we don't guess\n",
        "\n",
        "  pixelDensityIncrease = imageWidthGenerated / imageWidthCropped # could be 1.25 which is 25% larger\n",
        " \n",
        "  if useRealESRGANforUpscale == False:\n",
        "    # easy way to unify: downscale the upscaled crop to fit the original crop:\n",
        "    imagePathFull = Image.open(FRpathProjectUprezzedCrops + image)\n",
        "    new_image = imagePathFull.resize((imageWidthCropped, imageWidthCropped))\n",
        "  elif useRealESRGANforUpscale == True:\n",
        "    # harder way to unify; upscale\n",
        "    # although since we used this variable to swap the original input images for the upscaled ones, it might be easier than I think\n",
        "    imagePathFull = Image.open(FRpathProjectUprezzedCrops + image)\n",
        "    new_image = imagePathFull.resize((imageWidthCropped, imageWidthCropped))\n",
        "\n",
        "  #print(image)\n",
        "  #print(imgPathInput)\n",
        "  #print(imgPath + imgPathInput[counter])\n",
        "  # -------------------------------------------------------------------------------\n",
        "  # -------------------------FACE RECOGNITION: ----------------------------------\n",
        "  # D) merge cropped image back to the original: ---------------------------------------------------\n",
        "\n",
        "  # this code will take 2 images (1 full and 1 crop) and paste the cropped image at proper coordinates\n",
        "  #Read the two images:\n",
        "  # image1 = Image.open('/content/known/y1.png')\n",
        "  # image1 = Image.open(imgPath + imgPathInput[counter])\n",
        "  # to get image1 which is the full image which may not correspond to the crop #, we have to determine which initial images should go with this crop:\n",
        "  currentInitialImageWork = countOfFacesPerInput[selectInitialImage] # example: [0, 1, 6]\n",
        "  # select which initial image we're on:\n",
        "  while currentInitialImageWork == 0:\n",
        "    if currentInitialImageWork == 0:\n",
        "      # this image has no faces, so we should not process on this initial image\n",
        "      selectInitialImage += 1\n",
        "      currentInitialImageWork = countOfFacesPerInput[selectInitialImage] # example: [0, 1, 6]\n",
        "    if currentInitialImageWork != 0: # if this image has a face, we can stop looking for an image with a face\n",
        "      break\n",
        "  image1 = Image.open(imgPath + imgPathInput[selectInitialImage]) # the initial base image\n",
        "  countRepeatPrompts += 1\n",
        "  image2 = new_image # the cropped image\n",
        "  # image2 = image2.convert(\"RGBA\") # make sure the image supports transparency\n",
        "  # now we have the images to process with ----\n",
        "\n",
        "\n",
        "  #'''\n",
        "  # -----------------------------\n",
        "  # I have new code for creating cleaner transparent crops which will blend better when composited:\n",
        "  im_rgba = image2.copy()\n",
        "  im_rgba.putalpha(0)\n",
        "\n",
        "  mask_im = Image.new(\"L\", image2.size, 0)\n",
        "  mask_im_unpadded = mask_im\n",
        "  draw = ImageDraw.Draw(mask_im)\n",
        "  drawUnpadded = ImageDraw.Draw(mask_im_unpadded)\n",
        "  x, y = image2.size # height and width of the crop image\n",
        "  distance = y / 12 # framing of mask that will contain the blur region\n",
        "  draw.rectangle((int(distance), int(distance), positionFaceImageWList[counter]-int(distance), positionFaceImageHList[counter]-int(distance)), fill=255)\n",
        "  # create a blur for the mask:\n",
        "  mask_im_blur = mask_im.filter(ImageFilter.GaussianBlur(int(distance/2)))\n",
        "  # for drawing the unpadded mask:\n",
        "  draw.rectangle((0, 0, positionFaceImageWList[counter], positionFaceImageHList[counter]), fill=255)\n",
        "  \n",
        "\n",
        "  counterString = str(counter+1)\n",
        "  stringImageNumber = str(selectInitialImage+1)\n",
        "  stringcountRepeatPrompts = str(countRepeatPrompts)\n",
        "  imgPathOutput = FRpathProjectExtended + \"extended_\" + stringImageNumber.zfill(4) + \"_f\" + stringcountRepeatPrompts.zfill(2) + \".png\" # ex. finished_0001.png\n",
        "\n",
        "  # copy the bg transparent:\n",
        "  x, y = image1.size # height and width of the base image\n",
        "  new_imageTransparent = Image.new(\"RGBA\",(x,y),(0,0,0,0)).convert(\"RGBA\") # full transparency of the input image\n",
        "  new_imageTransparent.paste(image2, (positionFaceImageXList[counter],positionFaceImageYList[counter]), mask_im_unpadded)\n",
        "  new_imageTransparent.putalpha(0)\n",
        "  new_imageTransparent.paste(image2, (positionFaceImageXList[counter],positionFaceImageYList[counter]), mask_im_blur)\n",
        "  new_imageTransparent.save(imgPathOutput) # the transparent cropped images\n",
        "  # ------ end new code for cleaner results -------------\n",
        "  #'''\n",
        "  new_image = image1.copy() # a copy of the original image\n",
        "  new_image.paste(image2, (positionFaceImageXList[counter],positionFaceImageYList[counter]), mask_im_blur)\n",
        "\n",
        "\n",
        "  tempCollectExtendedImg.append(new_imageTransparent) # add this new extended crop to an array so we can apply it to the full image after processing it\n",
        "  counter +=1\n",
        "\n",
        "  # counter for the base image selection / image1:\n",
        "  # basically this section runs when we have finished processing a base image and ready to merge the generation crops finally with the base!\n",
        "  if countRepeatPrompts == countOfFacesPerInput[selectInitialImage]: # once we're run through all the crops for this intiial image, we have to move to the next one\n",
        "    selectInitialImage += 1\n",
        "    newMerge = new_image.copy() # temp save for image to be pasted on\n",
        "    # at this point we also create the final result by merging the collected extended images with the input image (image1):\n",
        "    imgPathOutputFin = FRpathProjectFinal + \"final_\" + stringImageNumber.zfill(4) + \"_f\" + stringcountRepeatPrompts.zfill(2) + \".png\" # ex. finished_0001.png\n",
        "\n",
        "    for extendedImg in tempCollectExtendedImg:\n",
        "      newMerge.paste(extendedImg,(0,0), extendedImg)\n",
        "      newMerge = newMerge.copy()\n",
        "      newMerge.save(imgPathOutputFin)\n",
        "    countRepeatPrompts = 0\n",
        "    tempCollectExtendedImg.clear() # get ready for appending the next round of crops\n",
        "  # continue until for loop is done...\n",
        "\n",
        "# ----------------------END FACIAL RECOGNITION P2 -------------------------------\n",
        "# -------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Now we copy the final files to Google Drive to back them up:\n",
        "if len(init_imgs_list) != 0:\n",
        "  print(\"Wait while copying to Google Drive...\")\n",
        "  copyAddressOnDrive = \"/content/gdrive/MyDrive/AI/StableDiffusionUpscaleFaces/\" + ProjectName\n",
        "\n",
        "  directoryToCopy1 = FRpathProjectFinal # final images\n",
        "  directoryToCopy2 = FRpathProjectUpscaledINIT # upscaled INIT images\n",
        "  directoryToCopy3 = FRpathProjectExtended # extended crops for photobashing\n",
        "  directoryToCopy4 = FRpathProjectUprezzedCrops # extended crops for photobashing\n",
        "  listOfImagesToCopy1 = [f for f in listdir(directoryToCopy1) if f.endswith(\".png\") and isfile(join(directoryToCopy1, f))]\n",
        "  listOfImagesToCopy1.sort()\n",
        "  listOfImagesToCopy2 = [f for f in listdir(directoryToCopy2) if f.endswith(\".png\") and isfile(join(directoryToCopy2, f))]\n",
        "  listOfImagesToCopy2.sort()\n",
        "  listOfImagesToCopy3 = [f for f in listdir(directoryToCopy3) if f.endswith(\".png\") and isfile(join(directoryToCopy3, f))]\n",
        "  listOfImagesToCopy3.sort()\n",
        "  listOfImagesToCopy4 = [f for f in listdir(directoryToCopy4) if f.endswith(\".png\") and isfile(join(directoryToCopy4, f))]\n",
        "  listOfImagesToCopy4.sort()\n",
        "\n",
        "  # rename the files of the original source image before copying them:\n",
        "  dir_list = os.listdir(directoryToCopy1)\n",
        "  filename_without_ext = []\n",
        "  for x in dir_list:\n",
        "    filename_without_ext.append(os.path.splitext(x)[0])\n",
        "    print(filename_without_ext)\n",
        "    filename_without_ext.sort()\n",
        "\n",
        "  count=0\n",
        "  for index, file in enumerate(listOfImagesToCopy2):\n",
        "    valueSt = str(index+1) \n",
        "    # newName = \"upscaledInput_\" + valueSt.zfill(4)\n",
        "    newName = filename_without_ext[count] + \"_sourceIMG\"\n",
        "    os.rename(os.path.join(directoryToCopy2, file), os.path.join(directoryToCopy2, ''.join([newName, '.png'])))\n",
        "    count += 1  \n",
        "  listOfImagesToCopy2 = [f for f in listdir(directoryToCopy2) if f.endswith(\".png\") and isfile(join(directoryToCopy2, f))]\n",
        "  listOfImagesToCopy2.sort()\n",
        "\n",
        "  isExist = os.path.exists(copyAddressOnDrive + \"/001\")\n",
        "  if not isExist:\n",
        "      os.makedirs(copyAddressOnDrive + \"/001\")\n",
        "      copyAddressOnDrive = copyAddressOnDrive + \"/001\"\n",
        "  else:\n",
        "    i=1\n",
        "    while os.path.exists(copyAddressOnDrive + \"/\" + str(i).zfill(3)):\n",
        "      i+=1\n",
        "    os.makedirs(copyAddressOnDrive + \"/\" + str(i).zfill(3))\n",
        "    copyAddressOnDrive = copyAddressOnDrive + \"/\" + str(i).zfill(3)\n",
        "\n",
        "  for x in listOfImagesToCopy1:\n",
        "    fullPath = directoryToCopy1 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  for x in listOfImagesToCopy2:\n",
        "    fullPath = directoryToCopy2 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  for x in listOfImagesToCopy3:\n",
        "    fullPath = directoryToCopy3 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  for x in listOfImagesToCopy4:\n",
        "    fullPath = directoryToCopy4 + x\n",
        "    shutil.copy(fullPath, copyAddressOnDrive)\n",
        "\n",
        "  print(\"Please wait for files to copy to Google Drive. Enjoy!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1z-Twr9G3A0b"
      },
      "outputs": [],
      "source": [
        "#@title Experimental Gender & Age Detection (study only):\n",
        "#@markdown `note: this script will not do more than study an image and guess the gender/age`<br>\n",
        "#@markdown `Gender detection is fine, but the age detection is way off`<br>\n",
        "#@markdown https://data-flair.training/blogs/image-segmentation-machine-learning/\n",
        "\n",
        "\n",
        "# models are required to be added manually to /content/gad/\n",
        "# models are from here: https://data-flair.training/blogs/python-project-gender-age-detection/\n",
        "# this code was written and taken from here: https://dev.to/ethand91/simple-age-and-gender-detection-using-python-and-opencv-319h\n",
        "\n",
        "\n",
        "# Gender and Age Detection Code Fragment\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Defined the model files\n",
        "pathStart = \"/content/gad/\"\n",
        "FACE_PROTO = pathStart + \"opencv_face_detector.pbtxt\"\n",
        "FACE_MODEL = pathStart + \"opencv_face_detector_uint8.pb\"\n",
        "\n",
        "AGE_PROTO = pathStart + \"age_deploy.prototxt\"\n",
        "AGE_MODEL = pathStart + \"age_net.caffemodel\"\n",
        "\n",
        "GENDER_PROTO = pathStart + \"gender_deploy.prototxt\"\n",
        "GENDER_MODEL = pathStart + \"gender_net.caffemodel\"\n",
        "\n",
        "# Load network\n",
        "FACE_NET = cv2.dnn.readNet(FACE_MODEL, FACE_PROTO)\n",
        "AGE_NET = cv2.dnn.readNet(AGE_MODEL, AGE_PROTO)\n",
        "GENDER_NET = cv2.dnn.readNet(GENDER_MODEL, GENDER_PROTO)\n",
        "\n",
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "AGE_LIST = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
        "GENDER_LIST = [\"Male\", \"Female\"]\n",
        "\n",
        "box_padding = 20\n",
        "\n",
        "def get_face_box (net, frame, conf_threshold = 0.7):\n",
        "  frame_copy = frame.copy()\n",
        "  frame_height = frame_copy.shape[0]\n",
        "  frame_width = frame_copy.shape[1]\n",
        "  blob = cv2.dnn.blobFromImage(frame_copy, 1.0, (300, 300), [104, 117, 123], True, False)\n",
        "\n",
        "  net.setInput(blob)\n",
        "  detections = net.forward()\n",
        "  boxes = []\n",
        "\n",
        "  for i in range(detections.shape[2]):\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    if confidence > conf_threshold:\n",
        "      x1 = int(detections[0, 0, i, 3] * frame_width)\n",
        "      y1 = int(detections[0, 0, i, 4] * frame_height)\n",
        "      x2 = int(detections[0, 0, i, 5] * frame_width)\n",
        "      y2 = int(detections[0, 0, i, 6] * frame_height)\n",
        "      boxes.append([x1, y1, x2, y2])\n",
        "      cv2.rectangle(frame_copy, (x1, y1), (x2, y2), (0, 255, 0), int(round(frame_height / 150)), 8)\n",
        "\n",
        "  return frame_copy, boxes\n",
        "\n",
        "def age_gender_detector (input_path):\n",
        "  image = cv2.imread(input_path)\n",
        "  frame = image.copy()\n",
        "  frame_face, boxes = get_face_box(FACE_NET, frame)\n",
        "\n",
        "  for box in boxes:\n",
        "    face = frame[max(0, box[1] - box_padding):min(box[3] + box_padding, frame.shape[0] - 1), \\\n",
        "      max(0, box[0] - box_padding):min(box[2] + box_padding, frame.shape[1] - 1)]\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB = False)\n",
        "    GENDER_NET.setInput(blob)\n",
        "    gender_predictions = GENDER_NET.forward()\n",
        "    gender = GENDER_LIST[gender_predictions[0].argmax()]\n",
        "    print(\"Gender: {}, conf: {:.3f}\".format(gender, gender_predictions[0].max()))\n",
        "\n",
        "    AGE_NET.setInput(blob)\n",
        "    age_predictions = AGE_NET.forward()\n",
        "    age = AGE_LIST[age_predictions[0].argmax()]\n",
        "    print(\"Age: {}, conf: {:.3f}\".format(age, age_predictions[0].max()))\n",
        "\n",
        "    label = \"{},{}\".format(gender, age)\n",
        "    cv2.putText(frame_face, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "  return frame_face\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  inputImageFullPath = \"/content/init_image/a0011.png\" #@param {type:\"string\"}\n",
        "  output = age_gender_detector(inputImageFullPath)\n",
        "  cv2.imwrite(\"output.jpg\", output)\n",
        "  cv2_imshow(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3R1pX2brMPF"
      },
      "source": [
        "## C) Upscale from File Browser (Real ESRGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L6GCJDkYrXCW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "#@markdown `note: run this to upres from selecting file browser`<br>\n",
        "#@markdown `disclaimer: if you get a CUDA warning even with low settings, you'll have to disconnect & restart runtime`<br>\n",
        "#1) set up the file to load into the ESRGAN:\n",
        "upload_folder = 'upload' # wierd directories that ESRGAN uses\n",
        "result_folder = 'results'\n",
        "scaleMultiplier = 2 #@param {type:\"slider\", min:1.5, max:4, step:0.5}\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "\n",
        "# upload images\n",
        "# upload = \"/content/1.jpg\" #@param {type:\"string\"}\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(upload_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.copy(filename, dst_path)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "# 2) Core Code to Make ESRGAN Uprez the Image:\n",
        "# if it is out of memory, try to use the `--tile` option\n",
        "# We upsample the image with the scale factor X3.5\n",
        "if scaleMultiplier == 1.5:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 1.5 --face_enhance\n",
        "elif scaleMultiplier == 2:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2 --face_enhance\n",
        "elif scaleMultiplier == 2.5:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 2.5 --face_enhance\n",
        "elif scaleMultiplier == 3:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3 --face_enhance\n",
        "elif scaleMultiplier == 3.5:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 3.5 --face_enhance\n",
        "elif scaleMultiplier == 4:\n",
        "  !python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale 4 --face_enhance\n",
        "\n",
        "\n",
        "# Arguments\n",
        "# -n, --model_name: Model names\n",
        "# -i, --input: input folder or image\n",
        "# --outscale: Output scale, can be arbitrary scale factore. \n",
        "# ---------------------------------------------------------------------------------------------\n",
        "\n",
        "input_folder = '/content/stable-diffusion/Real-ESRGAN/upload'\n",
        "result_folder = '/content/stable-diffusion/Real-ESRGAN/results'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "output_list.sort()\n",
        "# print(input_list)\n",
        "print(\"saved to:\", output_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eWGl_J8krgvx"
      },
      "outputs": [],
      "source": [
        "#@markdown `Download the Upscaled Images as .zip`<br>\n",
        "\n",
        "# Make a zip file of your results to download:\n",
        "from google.colab import files\n",
        "import os\n",
        "file_path = \"/content/stable-diffusion/Real-ESRGAN/results.zip\"\n",
        "if os.path.isfile(file_path):\n",
        "  os.remove(file_path)\n",
        "!zip -r results.zip /content/stable-diffusion/Real-ESRGAN/results\n",
        "files.download('results.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvEQDBd2cNh7"
      },
      "source": [
        "## <u>Utility:</u> Clear init_image Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W84nCpoKVP_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "files = glob.glob('/content/init_image/**/*', recursive=True)\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s : %s\" % (f, e.strerror))\n",
        "Info = 'run to clear image folder' #@param [\"run to clear image folder\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-vPBA3MwTxO"
      },
      "outputs": [],
      "source": [
        "function ClickConnect(){\n",
        "{\n",
        "console.log(“Working”);\n",
        "document.querySelector(“colab-connect-button”).shadowRoot.getElementById(‘connect’).click();\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "UU52ZvES6-1T",
        "0GOr30k9Gh1L",
        "R7KPSsU8zhut",
        "rnavTMQkkUO-",
        "aGldtYOMb5fk",
        "rGlb8rzxcB7F",
        "o8hwXeJhIfJl",
        "lx5EjEc5bgRV",
        "xzsZgcm37USQ",
        "vqCQrrKma9HA",
        "o3R1pX2brMPF",
        "nvEQDBd2cNh7"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "669338669e324aff9b60eae686382049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_663964ff6cd6450eb0f63c11b8b88b02",
              "IPY_MODEL_ad2b4c004e30431c8c170ac4785711ba",
              "IPY_MODEL_bba8f8f59d8741f997f9678fde15c6f2"
            ],
            "layout": "IPY_MODEL_e61b6c43495c4eb9bf8f67e92f554944"
          }
        },
        "663964ff6cd6450eb0f63c11b8b88b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ceb6a49daa8452ba59e48d173bdf85d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0219a3018f4c7c87c91ca2ea174c28",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "ad2b4c004e30431c8c170ac4785711ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ffbe7a80a4b408aaa8ca665f4279994",
            "max": 961143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4cc53220559474ebec5409a3a8821c7",
            "value": 961143
          }
        },
        "bba8f8f59d8741f997f9678fde15c6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dddb8c3e243b4a4da5bb0f4db1a93e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_5cfc9fcada99451cbef6f8c3a8deebc0",
            "value": " 939k/939k [00:00&lt;00:00, 2.02MB/s]"
          }
        },
        "e61b6c43495c4eb9bf8f67e92f554944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ceb6a49daa8452ba59e48d173bdf85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0219a3018f4c7c87c91ca2ea174c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ffbe7a80a4b408aaa8ca665f4279994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4cc53220559474ebec5409a3a8821c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dddb8c3e243b4a4da5bb0f4db1a93e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfc9fcada99451cbef6f8c3a8deebc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f423adc48b84d29b71e7a77fc2624bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68e431306ccb484685ba01dcd3ed553f",
              "IPY_MODEL_9662f319d8ef498a95bb29409256fcd6",
              "IPY_MODEL_3f340c3fe04d4699a911489a2ff26c24"
            ],
            "layout": "IPY_MODEL_5a50671e27c54d46bf64f3af0dfbca86"
          }
        },
        "68e431306ccb484685ba01dcd3ed553f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6ff2f87e824a148d106ee9c860d253",
            "placeholder": "​",
            "style": "IPY_MODEL_9e12c27425604668a71fbfe24a07e115",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "9662f319d8ef498a95bb29409256fcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36347058c5e0418c8d3333e4f06a6f3f",
            "max": 524619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_889b93649edd4802a92be13c69615997",
            "value": 524619
          }
        },
        "3f340c3fe04d4699a911489a2ff26c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351251aae3674fa4a98795c45e793f74",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7bcc25d9fd4da3b1bffdc5cc772e1e",
            "value": " 512k/512k [00:00&lt;00:00, 626kB/s]"
          }
        },
        "5a50671e27c54d46bf64f3af0dfbca86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6ff2f87e824a148d106ee9c860d253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e12c27425604668a71fbfe24a07e115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36347058c5e0418c8d3333e4f06a6f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889b93649edd4802a92be13c69615997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "351251aae3674fa4a98795c45e793f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7bcc25d9fd4da3b1bffdc5cc772e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8071b9d736f64a39b2728981f21a232f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_162a956fca16418bbc30ea43de7725a0",
              "IPY_MODEL_1e8bfe57bd274bd0821beabe1717e5c1",
              "IPY_MODEL_f13bd986b7144799b4cb8aaf9230a547"
            ],
            "layout": "IPY_MODEL_fc27fc3e511844ba82057516811a2a28"
          }
        },
        "162a956fca16418bbc30ea43de7725a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a5eecd365c417f85d9da8e886fdc86",
            "placeholder": "​",
            "style": "IPY_MODEL_f97f7f85350b49b6a0a059b86322b7a6",
            "value": "Downloading special_tokens_map.json: 100%"
          }
        },
        "1e8bfe57bd274bd0821beabe1717e5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3973866f8c3a44dfaea0a17eddbbde76",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3772a7d47420441bbc1b311c42590142",
            "value": 389
          }
        },
        "f13bd986b7144799b4cb8aaf9230a547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7bf2f1905b349089a2908e0bf6179e9",
            "placeholder": "​",
            "style": "IPY_MODEL_6a01e1743e194efaa0cb5ce5de93c045",
            "value": " 389/389 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "fc27fc3e511844ba82057516811a2a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a5eecd365c417f85d9da8e886fdc86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97f7f85350b49b6a0a059b86322b7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3973866f8c3a44dfaea0a17eddbbde76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3772a7d47420441bbc1b311c42590142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7bf2f1905b349089a2908e0bf6179e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a01e1743e194efaa0cb5ce5de93c045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9beda51032b5432fbe846ac121b425cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_166bd2a1b47445aca1e7656b3d1dcd19",
              "IPY_MODEL_bf616f2afeb8401fbd25ee7bc40d0ff1",
              "IPY_MODEL_449fc373eca54ea185c5de9737bdfb3c"
            ],
            "layout": "IPY_MODEL_1bdfdd5f7f814f818cb8ae15a99646d2"
          }
        },
        "166bd2a1b47445aca1e7656b3d1dcd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1afd5a2727a041d79d4e4593b316db41",
            "placeholder": "​",
            "style": "IPY_MODEL_a49b55f740c54c668bf6e2f6a83f1e72",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "bf616f2afeb8401fbd25ee7bc40d0ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a9e781f46443af857f60af0f1a2cb0",
            "max": 905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70f45ca57f2546c6a702d9fb1b37a168",
            "value": 905
          }
        },
        "449fc373eca54ea185c5de9737bdfb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b44424773f904770b4d9751e66609178",
            "placeholder": "​",
            "style": "IPY_MODEL_7c446cdf78eb430dbea34e500bd29f3d",
            "value": " 905/905 [00:00&lt;00:00, 33.9kB/s]"
          }
        },
        "1bdfdd5f7f814f818cb8ae15a99646d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afd5a2727a041d79d4e4593b316db41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a49b55f740c54c668bf6e2f6a83f1e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01a9e781f46443af857f60af0f1a2cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f45ca57f2546c6a702d9fb1b37a168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b44424773f904770b4d9751e66609178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c446cdf78eb430dbea34e500bd29f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eedf09703b674a879a220c4ee09806ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_724a8d58b76945e6a2d3c8c720c1ed4c",
              "IPY_MODEL_18ec5a7e730046fca9d8e98560efa18b",
              "IPY_MODEL_bda1fce5aa9444faa1d87967fbddb8b3"
            ],
            "layout": "IPY_MODEL_03e6abc874c34d4390eb6f8623771c5f"
          }
        },
        "724a8d58b76945e6a2d3c8c720c1ed4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2fcd78b605d4ef4b9598d9194923e15",
            "placeholder": "​",
            "style": "IPY_MODEL_59f3b82cf21042179e95181bab4220b9",
            "value": "Downloading config.json: 100%"
          }
        },
        "18ec5a7e730046fca9d8e98560efa18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_629168bf5ecb42deb0921e14213c9ef6",
            "max": 4519,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5886a1a01a7e4fb6b993aafcdabb0a68",
            "value": 4519
          }
        },
        "bda1fce5aa9444faa1d87967fbddb8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8645c4941bf34aaa8a1e487bc14bf235",
            "placeholder": "​",
            "style": "IPY_MODEL_00fa9bee8da34747abb255cb0f3b52ad",
            "value": " 4.41k/4.41k [00:00&lt;00:00, 131kB/s]"
          }
        },
        "03e6abc874c34d4390eb6f8623771c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fcd78b605d4ef4b9598d9194923e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f3b82cf21042179e95181bab4220b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "629168bf5ecb42deb0921e14213c9ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5886a1a01a7e4fb6b993aafcdabb0a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8645c4941bf34aaa8a1e487bc14bf235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00fa9bee8da34747abb255cb0f3b52ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51abe0445d7d407898bc713fcdae648c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3e0bf22f57c4382989b63c3c5dda941",
              "IPY_MODEL_4edb5d612a64451ca2fe9202f3d46c4e",
              "IPY_MODEL_64ff97d93f834346ba374ac0e3a3603a"
            ],
            "layout": "IPY_MODEL_cd957ff1cdae42e6a07c48b21a99a1a5"
          }
        },
        "a3e0bf22f57c4382989b63c3c5dda941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cdcbcee5b9549a4a2f30c0df8802aa6",
            "placeholder": "​",
            "style": "IPY_MODEL_e1a4d9fbf5e2453889487632c4992e04",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "4edb5d612a64451ca2fe9202f3d46c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b1074628004a5ab120b1158c0ddb4c",
            "max": 1710671599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abbaa6f5ee6c4184b7b983ca1d7c0155",
            "value": 1710671599
          }
        },
        "64ff97d93f834346ba374ac0e3a3603a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef1cf3b3ad443dabd7504c5ab546ac2",
            "placeholder": "​",
            "style": "IPY_MODEL_59694b308c28462a93fea3c7a3c33b98",
            "value": " 1.59G/1.59G [00:33&lt;00:00, 67.3MB/s]"
          }
        },
        "cd957ff1cdae42e6a07c48b21a99a1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cdcbcee5b9549a4a2f30c0df8802aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a4d9fbf5e2453889487632c4992e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1b1074628004a5ab120b1158c0ddb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbaa6f5ee6c4184b7b983ca1d7c0155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fef1cf3b3ad443dabd7504c5ab546ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59694b308c28462a93fea3c7a3c33b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}